  0%|                                                                                                    | 0/45 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  2%|██                                                                                          | 1/45 [00:26<19:21, 26.40s/it]
{'loss': 8.8598, 'grad_norm': 570.8169036565052, 'learning_rate': 1.9555555555555556e-06, 'epoch': 0.28}

  4%|████                                                                                        | 2/45 [00:47<16:47, 23.42s/it]

  7%|██████▏                                                                                     | 3/45 [01:08<15:39, 22.36s/it]


 11%|██████████▏                                                                                 | 5/45 [01:50<14:23, 21.58s/it]
{'loss': 5.8354, 'grad_norm': 120.54708056528163, 'learning_rate': 1.7777777777777775e-06, 'epoch': 1.4}


 16%|██████████████▎                                                                             | 7/45 [02:33<13:29, 21.31s/it]
{'loss': 5.7605, 'grad_norm': 109.96884768395775, 'learning_rate': 1.6888888888888888e-06, 'epoch': 1.96}

 18%|████████████████▎                                                                           | 8/45 [02:54<13:06, 21.26s/it]

 20%|██████████████████▍                                                                         | 9/45 [03:15<12:44, 21.24s/it]

 22%|████████████████████▏                                                                      | 10/45 [03:36<12:22, 21.20s/it]

 24%|██████████████████████▏                                                                    | 11/45 [03:57<11:58, 21.14s/it]

 27%|████████████████████████▎                                                                  | 12/45 [04:18<11:38, 21.16s/it]

 29%|██████████████████████████▎                                                                | 13/45 [04:39<11:16, 21.13s/it]

 31%|████████████████████████████▎                                                              | 14/45 [05:00<10:54, 21.11s/it]

 33%|██████████████████████████████▎                                                            | 15/45 [05:21<10:31, 21.06s/it]

 36%|████████████████████████████████▎                                                          | 16/45 [05:43<10:11, 21.09s/it]

 38%|██████████████████████████████████▍                                                        | 17/45 [06:04<09:50, 21.08s/it]

 40%|████████████████████████████████████▍                                                      | 18/45 [06:25<09:29, 21.07s/it]


 44%|████████████████████████████████████████▍                                                  | 20/45 [07:07<08:47, 21.12s/it]
{'loss': 4.0308, 'grad_norm': 72.51674779619877, 'learning_rate': 1.111111111111111e-06, 'epoch': 5.61}


 49%|████████████████████████████████████████████▍                                              | 22/45 [07:49<08:05, 21.10s/it]
{'loss': 3.8675, 'grad_norm': 63.70452192176981, 'learning_rate': 1.0222222222222221e-06, 'epoch': 6.18}


 53%|████████████████████████████████████████████████▌                                          | 24/45 [08:31<07:22, 21.09s/it]
{'loss': 3.9005, 'grad_norm': 68.3017274786114, 'learning_rate': 9.333333333333333e-07, 'epoch': 6.74}

 56%|██████████████████████████████████████████████████▌                                        | 25/45 [08:52<07:01, 21.09s/it]

 58%|████████████████████████████████████████████████████▌                                      | 26/45 [09:13<06:40, 21.08s/it]

 60%|██████████████████████████████████████████████████████▌                                    | 27/45 [09:34<06:19, 21.08s/it]

 62%|████████████████████████████████████████████████████████▌                                  | 28/45 [09:55<05:58, 21.06s/it]


 67%|███████████████████████████████████████████████████████████████████████▎                                   | 30/45 [10:37<05:15, 21.01s/it]
{'loss': 3.5055, 'grad_norm': 70.77695558164571, 'learning_rate': 6.666666666666666e-07, 'epoch': 8.42}


 71%|████████████████████████████████████████████████████████████████████████████                               | 32/45 [11:19<04:32, 20.98s/it]
{'loss': 3.5412, 'grad_norm': 54.82948755075252, 'learning_rate': 5.777777777777777e-07, 'epoch': 8.98}


 76%|████████████████████████████████████████████████████████████████████████████████▊                          | 34/45 [12:01<03:50, 20.99s/it]
{'loss': 3.4247, 'grad_norm': 50.798487459027754, 'learning_rate': 4.888888888888889e-07, 'epoch': 9.54}


 80%|█████████████████████████████████████████████████████████████████████████████████████▌                     | 36/45 [12:43<03:08, 20.97s/it]
{'loss': 3.3791, 'grad_norm': 62.62180318686935, 'learning_rate': 4e-07, 'epoch': 10.11}


 84%|██████████████████████████████████████████████████████████████████████████████████████████▎                | 38/45 [13:26<02:27, 21.07s/it]
{'loss': 3.3568, 'grad_norm': 52.59442540787355, 'learning_rate': 3.111111111111111e-07, 'epoch': 10.67}

 87%|████████████████████████████████████████████████████████████████████████████████████████████▋              | 39/45 [13:47<02:07, 21.17s/it]


 91%|█████████████████████████████████████████████████████████████████████████████████████████████████▍         | 41/45 [14:29<01:24, 21.15s/it]
{'loss': 3.4384, 'grad_norm': 62.72974765001351, 'learning_rate': 1.7777777777777776e-07, 'epoch': 11.51}


 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████▏    | 43/45 [15:11<00:42, 21.12s/it]
{'loss': 3.3543, 'grad_norm': 58.77961354713044, 'learning_rate': 8.888888888888888e-08, 'epoch': 12.07}

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [15:54<00:00, 21.08s/it][INFO|trainer.py:2383] 2024-08-30 07:20:36,409 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [15:54<00:00, 21.20s/it]
{'loss': 3.2077, 'grad_norm': 43.39936078563326, 'learning_rate': 0.0, 'epoch': 12.63}
{'train_runtime': 963.4024, 'train_samples_per_second': 14.013, 'train_steps_per_second': 0.047, 'train_loss': 4.3091242419348825, 'epoch': 12.63}
[INFO|trainer.py:3478] 2024-08-30 07:20:44,084 >> Saving model checkpoint to /model/output/llama2-13b-filtered_alpaca_1k_score_beautified-e15lr2e-06
[INFO|configuration_utils.py:472] 2024-08-30 07:20:44,087 >> Configuration saved in /model/output/llama2-13b-filtered_alpaca_1k_score_beautified-e15lr2e-06/config.json
[INFO|configuration_utils.py:769] 2024-08-30 07:20:44,087 >> Configuration saved in /model/output/llama2-13b-filtered_alpaca_1k_score_beautified-e15lr2e-06/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-30 07:21:04,527 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/llama2-13b-filtered_alpaca_1k_score_beautified-e15lr2e-06/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-30 07:21:04,529 >> tokenizer config file saved in /model/output/llama2-13b-filtered_alpaca_1k_score_beautified-e15lr2e-06/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-30 07:21:04,529 >> Special tokens file saved in /model/output/llama2-13b-filtered_alpaca_1k_score_beautified-e15lr2e-06/special_tokens_map.json
***** train metrics *****
  epoch                    =    12.6316
  total_flos               =     2661GF
  train_loss               =     4.3091
  train_runtime            = 0:16:03.40
  train_samples_per_second =     14.013
  train_steps_per_second   =      0.047
Figure saved at: /model/output/llama2-13b-filtered_alpaca_1k_score_beautified-e15lr2e-06/training_loss.png
08/30/2024 07:21:05 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
[INFO|trainer.py:3788] 2024-08-30 07:21:05,201 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-30 07:21:05,201 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-30 07:21:05,201 >>   Batch size = 1

 69%|██████████████████████████████████████████████████████████████████████████▊                                 | 9/13 [00:02<00:01,  3.13it/s]
***** eval metrics *****
  epoch                   =    12.6316
  eval_loss               =     1.6964
  eval_runtime            = 0:00:04.58
  eval_samples_per_second =     21.823
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  3.10it/s]
[INFO|modelcard.py:449] 2024-08-30 07:21:09,783 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}