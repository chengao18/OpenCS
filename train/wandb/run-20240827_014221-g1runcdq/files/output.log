  0%|                                                                                        | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  7%|█████▎                                                                          | 1/15 [00:45<10:35, 45.38s/it]

 13%|██████████▋                                                                     | 2/15 [01:26<09:16, 42.81s/it]
{'loss': 1.1917, 'grad_norm': 102.13869247253952, 'learning_rate': 1.7333333333333334e-06, 'epoch': 1.12}


 27%|█████████████████████▎                                                          | 4/15 [02:48<07:36, 41.52s/it]
{'loss': 1.0032, 'grad_norm': 44.27094286138412, 'learning_rate': 1.4666666666666665e-06, 'epoch': 2.25}


 40%|████████████████████████████████                                                | 6/15 [04:10<06:10, 41.22s/it]
{'loss': 0.8938, 'grad_norm': 20.527660038595638, 'learning_rate': 1.2e-06, 'epoch': 3.37}


 53%|██████████████████████████████████████████▋                                     | 8/15 [05:32<04:48, 41.19s/it]
{'loss': 0.8008, 'grad_norm': 9.351692217928646, 'learning_rate': 9.333333333333333e-07, 'epoch': 4.49}


 67%|████████████████████████████████████████████████████▋                          | 10/15 [06:54<03:25, 41.18s/it]

 73%|█████████████████████████████████████████████████████████▉                     | 11/15 [07:35<02:44, 41.14s/it]

 80%|███████████████████████████████████████████████████████████████▏               | 12/15 [08:17<02:03, 41.17s/it]

 87%|████████████████████████████████████████████████████████████████████▍          | 13/15 [08:58<01:22, 41.25s/it]

 93%|█████████████████████████████████████████████████████████████████████████▋     | 14/15 [09:40<00:41, 41.34s/it]
100%|███████████████████████████████████████████████████████████████████████████████| 15/15 [10:21<00:00, 41.33s/it][INFO|trainer.py:2383] 2024-08-27 01:52:50,947 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████| 15/15 [10:21<00:00, 41.43s/it]
{'loss': 0.726, 'grad_norm': 4.750554102652296, 'learning_rate': 0.0, 'epoch': 8.42}
{'train_runtime': 631.3452, 'train_samples_per_second': 21.383, 'train_steps_per_second': 0.024, 'train_loss': 0.8889264504114787, 'epoch': 8.42}
[INFO|trainer.py:3478] 2024-08-27 01:52:58,640 >> Saving model checkpoint to /model/output/mistral-7b-filtered_evol_instruct_1k_longest_beautified-e15lr2e-06
[INFO|configuration_utils.py:472] 2024-08-27 01:52:58,644 >> Configuration saved in /model/output/mistral-7b-filtered_evol_instruct_1k_longest_beautified-e15lr2e-06/config.json
[INFO|configuration_utils.py:769] 2024-08-27 01:52:58,644 >> Configuration saved in /model/output/mistral-7b-filtered_evol_instruct_1k_longest_beautified-e15lr2e-06/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-27 01:53:13,744 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/mistral-7b-filtered_evol_instruct_1k_longest_beautified-e15lr2e-06/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-27 01:53:13,744 >> tokenizer config file saved in /model/output/mistral-7b-filtered_evol_instruct_1k_longest_beautified-e15lr2e-06/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-27 01:53:13,745 >> Special tokens file saved in /model/output/mistral-7b-filtered_evol_instruct_1k_longest_beautified-e15lr2e-06/special_tokens_map.json
[INFO|trainer.py:3788] 2024-08-27 01:53:14,434 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-27 01:53:14,435 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-27 01:53:14,435 >>   Batch size = 1
  0%|                                                                                        | 0/13 [00:00<?, ?it/s]
***** train metrics *****
  epoch                    =     8.4211
  total_flos               =    11672GF
  train_loss               =     0.8889
  train_runtime            = 0:10:31.34
  train_samples_per_second =     21.383
  train_steps_per_second   =      0.024
Figure saved at: /model/output/mistral-7b-filtered_evol_instruct_1k_longest_beautified-e15lr2e-06/training_loss.png


 92%|████████████████████████████████████████████████████████████████████████▉      | 12/13 [00:04<00:00,  2.94it/s]
***** eval metrics *****
  epoch                   =     8.4211
  eval_loss               =     0.6639
  eval_runtime            = 0:00:04.75
  eval_samples_per_second =      21.04
100%|███████████████████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  2.98it/s]
[INFO|modelcard.py:449] 2024-08-27 01:53:19,187 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}