  0%|                                                                                                                    | 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|█                                                                                                         | 1/105 [00:37<1:05:10, 37.60s/it]
{'loss': 1.3193, 'grad_norm': 4.015740181425078, 'learning_rate': 9.090909090909091e-07, 'epoch': 0.14}


  2%|██                                                                                                          | 2/105 [01:07<56:34, 32.96s/it]
{'loss': 1.3369, 'grad_norm': 3.543669205540136, 'learning_rate': 2.7272727272727272e-06, 'epoch': 0.42}
  4%|████                                                                                                        | 4/105 [02:07<51:58, 30.87s/it]

  5%|█████▏                                                                                                      | 5/105 [02:36<50:50, 30.50s/it]

  6%|██████▏                                                                                                     | 6/105 [03:06<49:54, 30.25s/it]

  7%|███████▏                                                                                                    | 7/105 [03:35<48:53, 29.93s/it]
  7%|███████▏                                                                                                    | 7/105 [03:35<48:53, 29.93s/it][INFO|trainer.py:3478] 2024-08-08 14:08:08,162 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7
[INFO|configuration_utils.py:472] 2024-08-08 14:08:08,166 >> Configuration saved in /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7/config.json
[INFO|configuration_utils.py:769] 2024-08-08 14:08:08,167 >> Configuration saved in /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-08 14:08:26,099 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-08 14:08:26,100 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-08 14:08:26,101 >> Special tokens file saved in /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7/special_tokens_map.json
[2024-08-08 14:08:26,662] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7 is about to be saved!
[2024-08-08 14:08:26,671] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7/global_step7/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-08 14:08:26,672] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7/global_step7/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-08 14:08:26,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7/global_step7/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-08 14:08:26,687] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7/global_step7/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-08 14:09:02,520] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7/global_step7/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-08 14:09:02,521] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-alpaca-longest-1k/checkpoint-7/global_step7/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt