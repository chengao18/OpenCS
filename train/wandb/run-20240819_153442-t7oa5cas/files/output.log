  0%|                                                                                                            | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  7%|██████▋                                                                                             | 1/15 [00:25<05:52, 25.19s/it]

 13%|█████████████▎                                                                                      | 2/15 [00:46<04:55, 22.72s/it]

 20%|████████████████████                                                                                | 3/15 [01:06<04:19, 21.66s/it]
{'loss': 1.4623, 'grad_norm': 1.5043875580514305, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.84}

 27%|██████████████████████████▋                                                                         | 4/15 [01:26<03:52, 21.15s/it]


 40%|████████████████████████████████████████                                                            | 6/15 [02:07<03:05, 20.65s/it]

 47%|██████████████████████████████████████████████▋                                                     | 7/15 [02:27<02:43, 20.46s/it]

 53%|█████████████████████████████████████████████████████▎                                              | 8/15 [02:47<02:22, 20.38s/it]

 60%|████████████████████████████████████████████████████████████                                        | 9/15 [03:07<02:01, 20.28s/it]

 67%|██████████████████████████████████████████████████████████████████                                 | 10/15 [03:27<01:41, 20.20s/it]

 73%|████████████████████████████████████████████████████████████████████████▌                          | 11/15 [03:47<01:20, 20.15s/it]

 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 12/15 [04:08<01:00, 20.15s/it]

 87%|█████████████████████████████████████████████████████████████████████████████████████▊             | 13/15 [04:28<00:40, 20.18s/it]

 93%|████████████████████████████████████████████████████████████████████████████████████████████▍      | 14/15 [04:48<00:20, 20.12s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [05:08<00:00, 20.13s/it][INFO|trainer.py:2383] 2024-08-19 15:39:59,407 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [05:08<00:00, 20.56s/it]
{'loss': 1.1741, 'grad_norm': 0.7035836525263867, 'learning_rate': 0.0, 'epoch': 4.21}
{'train_runtime': 319.8384, 'train_samples_per_second': 14.07, 'train_steps_per_second': 0.047, 'train_loss': 1.330895972251892, 'epoch': 4.21}
[INFO|trainer.py:3478] 2024-08-19 15:40:06,673 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-filtered_alpaca_1k_longest_beautified-e5lr1e-5
[INFO|configuration_utils.py:472] 2024-08-19 15:40:06,677 >> Configuration saved in /model/output/Llama-2-7b-hf-filtered_alpaca_1k_longest_beautified-e5lr1e-5/config.json
[INFO|configuration_utils.py:769] 2024-08-19 15:40:06,677 >> Configuration saved in /model/output/Llama-2-7b-hf-filtered_alpaca_1k_longest_beautified-e5lr1e-5/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-19 15:40:21,190 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-filtered_alpaca_1k_longest_beautified-e5lr1e-5/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-19 15:40:21,191 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-filtered_alpaca_1k_longest_beautified-e5lr1e-5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-19 15:40:21,192 >> Special tokens file saved in /model/output/Llama-2-7b-hf-filtered_alpaca_1k_longest_beautified-e5lr1e-5/special_tokens_map.json
***** train metrics *****
  epoch                    =     4.2105
  total_flos               =     2332GF
  train_loss               =     1.3309
  train_runtime            = 0:05:19.83
  train_samples_per_second =      14.07
  train_steps_per_second   =      0.047
Figure saved at: /model/output/Llama-2-7b-hf-filtered_alpaca_1k_longest_beautified-e5lr1e-5/training_loss.png
08/19/2024 15:40:21 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
[INFO|trainer.py:3788] 2024-08-19 15:40:22,000 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-19 15:40:22,000 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-19 15:40:22,000 >>   Batch size = 1

 85%|███████████████████████████████████████████████████████████████████████████████████▊               | 11/13 [00:03<00:00,  3.27it/s]
***** eval metrics *****
  epoch                   =     4.2105
  eval_loss               =     1.6035
  eval_runtime            = 0:00:04.35
  eval_samples_per_second =     22.951
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.27it/s]
[INFO|modelcard.py:449] 2024-08-19 15:40:26,357 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}