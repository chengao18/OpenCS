  0%|                                                                                                            | 0/45 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.5717, 'grad_norm': 4.552276025297461, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.28}
  2%|██▏                                                                                                 | 1/45 [00:30<22:13, 30.30s/it]

  4%|████▍                                                                                               | 2/45 [00:53<18:34, 25.91s/it]

  7%|██████▋                                                                                             | 3/45 [01:15<17:06, 24.43s/it]

  9%|████████▉                                                                                           | 4/45 [01:38<16:16, 23.82s/it]

 11%|███████████                                                                                         | 5/45 [02:01<15:37, 23.43s/it]

 13%|█████████████▎                                                                                      | 6/45 [02:24<15:03, 23.17s/it]

 16%|███████████████▌                                                                                    | 7/45 [02:46<14:36, 23.05s/it]


 20%|█████████████████▊                                                                       | 9/45 [03:32<13:47, 23.00s/it]
{'loss': 1.2405, 'grad_norm': 0.7686355722754602, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.53}


 24%|█████████████████████▌                                                                  | 11/45 [04:18<13:03, 23.04s/it]

 27%|███████████████████████▍                                                                | 12/45 [04:42<12:45, 23.21s/it]
{'loss': 1.187, 'grad_norm': 0.8100856341906931, 'learning_rate': 7.333333333333333e-06, 'epoch': 3.37}

 29%|█████████████████████████▍                                                              | 13/45 [05:05<12:17, 23.04s/it]


 33%|█████████████████████████████▎                                                          | 15/45 [05:50<11:28, 22.96s/it]
{'loss': 1.014, 'grad_norm': 0.7374054896687049, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.21}

 36%|███████████████████████████████▎                                                        | 16/45 [06:14<11:06, 22.99s/it]


 40%|███████████████████████████████████▏                                                    | 18/45 [07:00<10:23, 23.08s/it]
{'loss': 1.004, 'grad_norm': 0.8107858696153691, 'learning_rate': 6e-06, 'epoch': 5.05}


 44%|███████████████████████████████████████                                                 | 20/45 [07:46<09:34, 22.96s/it]
{'loss': 0.8969, 'grad_norm': 0.7450106683389663, 'learning_rate': 5.555555555555557e-06, 'epoch': 5.61}

 47%|█████████████████████████████████████████                                               | 21/45 [08:09<09:12, 23.02s/it]


 51%|████████████████████████████████████████████▉                                           | 23/45 [08:54<08:22, 22.84s/it]

 53%|██████████████████████████████████████████████▉                                         | 24/45 [09:17<07:57, 22.73s/it]
{'loss': 0.7888, 'grad_norm': 0.7790675387504554, 'learning_rate': 4.666666666666667e-06, 'epoch': 6.74}

 56%|████████████████████████████████████████████████▉                                       | 25/45 [09:39<07:30, 22.54s/it]

 58%|██████████████████████████████████████████████████▊                                     | 26/45 [10:01<07:06, 22.43s/it]

 60%|████████████████████████████████████████████████████▊                                   | 27/45 [10:23<06:42, 22.35s/it]


 64%|████████████████████████████████████████████████████████▋                               | 29/45 [11:08<05:59, 22.45s/it]

 67%|██████████████████████████████████████████████████████████▋                             | 30/45 [11:31<05:36, 22.41s/it]
{'loss': 0.6643, 'grad_norm': 1.1800813838404864, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.42}

 69%|████████████████████████████████████████████████████████████▌                           | 31/45 [11:53<05:14, 22.48s/it]

 71%|██████████████████████████████████████████████████████████████▌                         | 32/45 [12:16<04:51, 22.45s/it]


 76%|██████████████████████████████████████████████████████████████████▍                     | 34/45 [13:00<04:06, 22.41s/it]

 78%|████████████████████████████████████████████████████████████████████▍                   | 35/45 [13:23<03:44, 22.41s/it]
{'loss': 0.5113, 'grad_norm': 0.8045659019666769, 'learning_rate': 2.222222222222222e-06, 'epoch': 9.82}

 80%|██████████████████████████████████████████████████████████████████████▍                 | 36/45 [13:45<03:21, 22.41s/it]

 82%|████████████████████████████████████████████████████████████████████████▎               | 37/45 [14:07<02:58, 22.37s/it]


 87%|████████████████████████████████████████████████████████████████████████████▎           | 39/45 [14:52<02:14, 22.45s/it]
{'loss': 0.4819, 'grad_norm': 0.8532007540091873, 'learning_rate': 1.3333333333333334e-06, 'epoch': 10.95}

 89%|██████████████████████████████████████████████████████████████████████████████▏         | 40/45 [15:15<01:52, 22.54s/it]


 93%|██████████████████████████████████████████████████████████████████████████████████▏     | 42/45 [16:01<01:07, 22.58s/it]

 96%|████████████████████████████████████████████████████████████████████████████████████    | 43/45 [16:23<00:44, 22.49s/it]

 98%|██████████████████████████████████████████████████████████████████████████████████████  | 44/45 [16:45<00:22, 22.45s/it]
{'loss': 0.4307, 'grad_norm': 0.7592654506328395, 'learning_rate': 2.2222222222222224e-07, 'epoch': 12.35}
{'loss': 0.4543, 'grad_norm': 0.8189834727761116, 'learning_rate': 0.0, 'epoch': 12.63}
100%|████████████████████████████████████████████████████████████████████████████████████████| 45/45 [17:07<00:00, 22.41s/it][INFO|trainer.py:2383] 2024-08-21 17:24:33,622 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|████████████████████████████████████████████████████████████████████████████████████████| 45/45 [17:07<00:00, 22.84s/it]
[INFO|trainer.py:3478] 2024-08-21 17:24:41,293 >> Saving model checkpoint to /model/output/llama2-7b-filtered_alpaca_1k_longest_beautified-e15lr1.0e-5
[INFO|configuration_utils.py:472] 2024-08-21 17:24:41,296 >> Configuration saved in /model/output/llama2-7b-filtered_alpaca_1k_longest_beautified-e15lr1.0e-5/config.json
[INFO|configuration_utils.py:769] 2024-08-21 17:24:41,296 >> Configuration saved in /model/output/llama2-7b-filtered_alpaca_1k_longest_beautified-e15lr1.0e-5/generation_config.json
***** train metrics *****
  epoch                    =    12.6316
  total_flos               =     7028GF
  train_loss               =     0.8645
  train_runtime            = 0:17:16.35
  train_samples_per_second =     13.026
  train_steps_per_second   =      0.043
Figure saved at: /model/output/llama2-7b-filtered_alpaca_1k_longest_beautified-e15lr1.0e-5/training_loss.png
08/21/2024 17:24:56 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
[INFO|modeling_utils.py:2698] 2024-08-21 17:24:55,556 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/llama2-7b-filtered_alpaca_1k_longest_beautified-e15lr1.0e-5/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-21 17:24:55,557 >> tokenizer config file saved in /model/output/llama2-7b-filtered_alpaca_1k_longest_beautified-e15lr1.0e-5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-21 17:24:55,558 >> Special tokens file saved in /model/output/llama2-7b-filtered_alpaca_1k_longest_beautified-e15lr1.0e-5/special_tokens_map.json
[INFO|trainer.py:3788] 2024-08-21 17:24:56,381 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-21 17:24:56,381 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-21 17:24:56,381 >>   Batch size = 1

100%|████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  3.19it/s]
[INFO|modelcard.py:449] 2024-08-21 17:25:00,875 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =    12.6316
  eval_loss               =     1.9972
  eval_runtime            = 0:00:04.49
  eval_samples_per_second =     22.256
  eval_steps_per_second   =      2.893