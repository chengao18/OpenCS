  0%|                                                                                                                     | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  3%|███▋                                                                                                         | 1/30 [00:29<14:17, 29.55s/it]
{'loss': 1.4457, 'grad_norm': 10.606333413186656, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.48}

  7%|███████▎                                                                                                     | 2/30 [00:54<12:33, 26.90s/it][INFO|trainer.py:3478] 2024-08-13 21:11:46,902 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2
[INFO|configuration_utils.py:472] 2024-08-13 21:11:46,905 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:11:46,905 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:12:06,024 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:12:06,027 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:12:06,027 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2/special_tokens_map.json
[2024-08-13 21:12:06,674] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2 is about to be saved!
[2024-08-13 21:12:06,683] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2/global_step2/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:12:06,683] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2/global_step2/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:12:06,696] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2/global_step2/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:12:06,699] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2/global_step2/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-13 21:12:25,809] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2/global_step2/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:12:25,810] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2/global_step2/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-13 21:12:27,995] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 1.109, 'grad_norm': 2.7233224762412602, 'learning_rate': 9e-06, 'epoch': 1.45}

 13%|█████████████████▌                                                                                                                  | 4/30 [02:35<17:13, 39.74s/it]
 13%|█████████████████▌                                                                                                                  | 4/30 [02:35<17:13, 39.74s/it][INFO|trainer.py:3478] 2024-08-13 21:13:29,687 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4
[INFO|configuration_utils.py:472] 2024-08-13 21:13:29,692 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:13:29,692 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:13:56,172 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:13:56,174 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:13:56,174 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4/special_tokens_map.json
[2024-08-13 21:13:57,151] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4 is about to be saved!
[2024-08-13 21:13:57,161] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4/global_step4/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:13:57,161] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4/global_step4/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:13:57,174] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4/global_step4/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:13:57,176] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4/global_step4/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-13 21:14:23,679] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4/global_step4/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:14:23,679] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4/global_step4/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[INFO|trainer.py:3570] 2024-08-13 21:14:29,236 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-2] due to args.save_total_limit
[2024-08-13 21:14:29,233] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 17%|██████████████████████                                                                                                              | 5/30 [04:22<26:34, 63.78s/it]

 20%|██████████████████████████▍                                                                                                         | 6/30 [04:46<20:10, 50.45s/it]
 20%|██████████████████████████▍                                                                                                         | 6/30 [04:46<20:10, 50.45s/it][INFO|trainer.py:3478] 2024-08-13 21:15:42,193 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6
[INFO|configuration_utils.py:472] 2024-08-13 21:15:42,196 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:15:42,197 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:16:07,415 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:16:07,416 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:16:07,417 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6/special_tokens_map.json
[2024-08-13 21:16:08,332] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6 is about to be saved!
[2024-08-13 21:16:08,341] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:16:08,341] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:16:08,354] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6/global_step6/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:16:08,357] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6/global_step6/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-13 21:16:36,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6/global_step6/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:16:36,422] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6/global_step6/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-13 21:16:40,531] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6 is ready now!
[INFO|trainer.py:3570] 2024-08-13 21:16:40,534 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-4] due to args.save_total_limit
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 23%|██████████████████████████████▊                                                                                                     | 7/30 [06:36<26:49, 69.97s/it]
{'loss': 0.9278, 'grad_norm': 1.3628066584009884, 'learning_rate': 7.666666666666667e-06, 'epoch': 3.39}

 27%|███████████████████████████████████▏                                                                                                | 8/30 [07:01<20:22, 55.57s/it][INFO|trainer.py:3478] 2024-08-13 21:17:54,462 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8
[INFO|configuration_utils.py:472] 2024-08-13 21:17:54,466 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:17:54,466 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:18:13,667 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:18:13,669 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:18:13,669 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8/special_tokens_map.json
[2024-08-13 21:18:14,413] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8 is about to be saved!
[2024-08-13 21:18:14,422] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8/global_step8/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:18:14,422] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8/global_step8/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:18:14,435] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8/global_step8/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:18:14,438] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8/global_step8/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|trainer.py:3570] 2024-08-13 21:18:47,034 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-6] due to args.save_total_limit
[2024-08-13 21:18:46,899] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8/global_step8/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:18:46,900] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8/global_step8/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-13 21:18:47,030] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.7759, 'grad_norm': 1.224997524336824, 'learning_rate': 7e-06, 'epoch': 4.36}
 30%|███████████████████████████████████████▌                                                                                            | 9/30 [08:37<23:53, 68.26s/it]

 33%|███████████████████████████████████████████▋                                                                                       | 10/30 [09:02<18:13, 54.69s/it][INFO|trainer.py:3478] 2024-08-13 21:19:59,541 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10
[INFO|configuration_utils.py:472] 2024-08-13 21:19:59,544 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:19:59,545 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:20:24,911 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:20:24,913 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:20:24,913 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10/special_tokens_map.json
[2024-08-13 21:20:25,879] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10 is about to be saved!
[2024-08-13 21:20:25,889] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:20:25,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:20:25,902] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10/global_step10/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:20:25,904] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|trainer.py:3570] 2024-08-13 21:20:59,167 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-8] due to args.save_total_limit
[2024-08-13 21:20:58,969] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:20:58,970] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10/global_step10/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-13 21:20:59,164] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 37%|████████████████████████████████████████████████                                                                                   | 11/30 [10:49<22:24, 70.77s/it]

 40%|████████████████████████████████████████████████████▍                                                                              | 12/30 [11:15<17:09, 57.19s/it]
 40%|████████████████████████████████████████████████████▍                                                                              | 12/30 [11:15<17:09, 57.19s/it][INFO|trainer.py:3478] 2024-08-13 21:22:12,029 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12
[INFO|configuration_utils.py:472] 2024-08-13 21:22:12,033 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:22:12,034 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:22:32,489 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:22:32,490 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:22:32,491 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12/special_tokens_map.json
[2024-08-13 21:22:33,352] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12 is about to be saved!
[2024-08-13 21:22:33,361] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:22:33,362] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:22:33,375] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12/global_step12/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:22:33,377] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12/global_step12/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-13 21:22:59,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12/global_step12/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:22:59,743] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12/global_step12/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[INFO|trainer.py:3570] 2024-08-13 21:23:05,445 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-10] due to args.save_total_limit
[2024-08-13 21:23:05,432] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 0.535, 'grad_norm': 1.1416068880529653, 'learning_rate': 5.666666666666667e-06, 'epoch': 6.3}

 47%|█████████████████████████████████████████████████████████████▏                                                                     | 14/30 [13:18<14:57, 56.09s/it]
 47%|█████████████████████████████████████████████████████████████▏                                                                     | 14/30 [13:18<14:57, 56.09s/it][INFO|trainer.py:3478] 2024-08-13 21:24:18,680 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14
[INFO|configuration_utils.py:472] 2024-08-13 21:24:18,683 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:24:18,684 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:24:44,312 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:24:44,314 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:24:44,314 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14/special_tokens_map.json
[2024-08-13 21:24:45,253] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step14 is about to be saved!
[2024-08-13 21:24:45,262] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14/global_step14/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:24:45,262] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14/global_step14/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:24:45,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14/global_step14/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:24:45,278] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14/global_step14/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-13 21:25:14,883] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14/global_step14/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:25:14,884] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14/global_step14/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[INFO|trainer.py:3570] 2024-08-13 21:25:18,007 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-12] due to args.save_total_limit
[2024-08-13 21:25:18,004] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 50%|█████████████████████████████████████████████████████████████████▌                                                                 | 15/30 [15:08<18:02, 72.17s/it]

 53%|█████████████████████████████████████████████████████████████████████▊                                                             | 16/30 [15:33<13:34, 58.18s/it]
 53%|█████████████████████████████████████████████████████████████████████▊                                                             | 16/30 [15:33<13:34, 58.18s/it][INFO|trainer.py:3478] 2024-08-13 21:26:31,715 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16
[INFO|configuration_utils.py:472] 2024-08-13 21:26:31,718 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:26:31,719 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:26:51,334 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:26:51,335 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:26:51,336 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16/special_tokens_map.json
[2024-08-13 21:26:52,190] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step16 is about to be saved!
[2024-08-13 21:26:52,199] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16/global_step16/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:26:52,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16/global_step16/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:26:52,212] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16/global_step16/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:26:52,215] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16/global_step16/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|trainer.py:3570] 2024-08-13 21:27:25,736 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-14] due to args.save_total_limit
[2024-08-13 21:27:25,412] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16/global_step16/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:27:25,413] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16/global_step16/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-13 21:27:25,732] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 57%|██████████████████████████████████████████████████████████████████████████▏                                                        | 17/30 [17:12<15:13, 70.26s/it]
{'loss': 0.3975, 'grad_norm': 1.0896616137498183, 'learning_rate': 4.333333333333334e-06, 'epoch': 8.24}

 60%|██████████████████████████████████████████████████████████████████████████████▌                                                    | 18/30 [17:36<11:18, 56.51s/it][INFO|trainer.py:3478] 2024-08-13 21:28:39,220 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18
[INFO|configuration_utils.py:472] 2024-08-13 21:28:39,223 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:28:39,224 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:29:04,541 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:29:04,543 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:29:04,543 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18/special_tokens_map.json
[2024-08-13 21:29:05,513] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step18 is about to be saved!
[2024-08-13 21:29:05,522] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:29:05,522] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:29:05,535] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18/global_step18/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:29:05,537] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18/global_step18/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|trainer.py:3570] 2024-08-13 21:30:44,610 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-16] due to args.save_total_limit
[2024-08-13 21:30:44,405] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18/global_step18/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:30:44,405] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18/global_step18/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-13 21:30:44,607] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 63%|██████████████████████████████████████████████████████████████████████████████████▉                                                | 19/30 [20:29<16:44, 91.35s/it]

 67%|███████████████████████████████████████████████████████████████████████████████████████▎                                           | 20/30 [20:53<11:52, 71.23s/it]
 67%|███████████████████████████████████████████████████████████████████████████████████████▎                                           | 20/30 [20:53<11:52, 71.23s/it][INFO|trainer.py:3478] 2024-08-13 21:31:53,849 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20
[INFO|configuration_utils.py:472] 2024-08-13 21:31:53,852 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:31:53,853 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:32:14,144 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:32:14,145 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:32:14,146 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20/special_tokens_map.json
[2024-08-13 21:32:15,005] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step20 is about to be saved!
[2024-08-13 21:32:15,014] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:32:15,015] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:32:15,028] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20/global_step20/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:32:15,030] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-13 21:32:40,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:32:40,336] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[INFO|trainer.py:3570] 2024-08-13 21:33:52,678 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-18] due to args.save_total_limit
[2024-08-13 21:33:52,674] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 70%|███████████████████████████████████████████████████████████████████████████████████████████▋                                       | 21/30 [23:36<14:48, 98.73s/it]

 73%|████████████████████████████████████████████████████████████████████████████████████████████████                                   | 22/30 [24:01<10:12, 76.52s/it]
 73%|████████████████████████████████████████████████████████████████████████████████████████████████                                   | 22/30 [24:01<10:12, 76.52s/it][INFO|trainer.py:3478] 2024-08-13 21:35:05,078 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22
[INFO|configuration_utils.py:472] 2024-08-13 21:35:05,082 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:35:05,082 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:35:30,785 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:35:30,787 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:35:30,787 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22/special_tokens_map.json
[2024-08-13 21:35:31,696] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step22 is about to be saved!
[2024-08-13 21:35:31,705] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22/global_step22/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:35:31,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22/global_step22/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:35:31,718] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22/global_step22/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:35:31,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22/global_step22/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-13 21:36:00,159] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22/global_step22/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:36:00,160] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22/global_step22/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[INFO|trainer.py:3570] 2024-08-13 21:37:09,172 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-20] due to args.save_total_limit
[2024-08-13 21:37:09,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 77%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                              | 23/30 [26:53<12:16, 105.24s/it]
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 24/30 [27:19<08:09, 81.57s/it]
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 24/30 [27:19<08:09, 81.57s/it]
[INFO|configuration_utils.py:769] 2024-08-13 21:38:22,467 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24/generation_config.json1:38:22,463 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24
[INFO|configuration_utils.py:769] 2024-08-13 21:38:22,467 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24/generation_config.json1:38:22,463 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24
[INFO|modeling_utils.py:2698] 2024-08-13 21:38:42,614 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:38:42,616 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:38:42,616 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24/special_tokens_map.json
[2024-08-13 21:38:43,472] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step24 is about to be saved!
[2024-08-13 21:38:43,481] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24/global_step24/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:38:43,481] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24/global_step24/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:38:43,494] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24/global_step24/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:38:43,497] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24/global_step24/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-13 21:39:10,517] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24/global_step24/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:39:10,518] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24/global_step24/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[INFO|trainer.py:3570] 2024-08-13 21:40:20,350 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-22] due to args.save_total_limit
[2024-08-13 21:40:20,346] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                     | 25/30 [30:01<08:48, 105.70s/it]
{'loss': 0.2059, 'grad_norm': 0.739440572866878, 'learning_rate': 1.6666666666666667e-06, 'epoch': 12.12}

 87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 26/30 [30:25<05:25, 81.28s/it][INFO|trainer.py:3478] 2024-08-13 21:41:32,701 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26
[INFO|configuration_utils.py:472] 2024-08-13 21:41:32,704 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:41:32,705 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:41:58,409 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:41:58,410 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:41:58,411 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26/special_tokens_map.json
[2024-08-13 21:41:59,342] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step26 is about to be saved!
[2024-08-13 21:41:59,351] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26/global_step26/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:41:59,351] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26/global_step26/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:41:59,364] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26/global_step26/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:41:59,367] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26/global_step26/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-13 21:43:34,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26/global_step26/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:43:34,027] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26/global_step26/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-13 21:43:34,707] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26 is ready now!
[INFO|trainer.py:3570] 2024-08-13 21:43:34,710 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-24] due to args.save_total_limit
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████             | 27/30 [33:15<05:23, 107.87s/it]

 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 28/30 [33:41<02:46, 83.27s/it]
 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎        | 28/30 [33:41<02:46, 83.27s/it][INFO|trainer.py:3478] 2024-08-13 21:44:47,761 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28
[INFO|configuration_utils.py:472] 2024-08-13 21:44:47,764 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:44:47,765 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:45:07,824 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:45:07,826 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:45:07,826 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28/special_tokens_map.json
[2024-08-13 21:45:08,686] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step28 is about to be saved!
[2024-08-13 21:45:08,695] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28/global_step28/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:45:08,695] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28/global_step28/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:45:08,708] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28/global_step28/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:45:08,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28/global_step28/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|trainer.py:3570] 2024-08-13 21:46:44,135 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-26] due to args.save_total_limit
[2024-08-13 21:46:44,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28/global_step28/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:46:44,036] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28/global_step28/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-08-13 21:46:44,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28 is ready now!
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 29/30 [36:23<01:46, 106.79s/it]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [36:47<00:00, 82.01s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [36:47<00:00, 82.01s/it][INFO|trainer.py:3478] 2024-08-13 21:47:38,062 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30
[INFO|configuration_utils.py:472] 2024-08-13 21:47:38,067 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:47:38,068 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:48:04,405 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:48:04,406 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:48:04,407 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/special_tokens_map.json
[2024-08-13 21:48:05,334] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step30 is about to be saved!
[2024-08-13 21:48:05,343] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:48:05,344] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:48:05,357] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:48:05,359] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-13 21:48:32,044] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:48:32,044] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[INFO|trainer.py:3570] 2024-08-13 21:49:40,680 >> Deleting older checkpoint [/model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-28] due to args.save_total_limit
[2024-08-13 21:49:40,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30 is ready now!
[INFO|trainer.py:3478] 2024-08-13 21:50:01,528 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30
[INFO|configuration_utils.py:472] 2024-08-13 21:50:01,532 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:50:01,533 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:50:36,930 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:50:36,932 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:50:36,933 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/special_tokens_map.json
[2024-08-13 21:50:37,825] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step30 is about to be saved!
[2024-08-13 21:50:37,834] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt
[2024-08-13 21:50:37,834] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2024-08-13 21:50:37,848] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2024-08-13 21:50:37,850] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-08-13 21:51:59,491] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-08-13 21:51:59,495] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/checkpoint-30/global_step30/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[INFO|trainer.py:2383] 2024-08-13 21:52:05,212 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [41:24<00:00, 82.82s/it]
[2024-08-13 21:52:05,208] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30 is ready now!
{'train_runtime': 2496.437, 'train_samples_per_second': 5.408, 'train_steps_per_second': 0.012, 'train_loss': 0.5602230752507845, 'epoch': 14.55}
[INFO|trainer.py:3478] 2024-08-13 21:52:15,537 >> Saving model checkpoint to /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5
[INFO|configuration_utils.py:472] 2024-08-13 21:52:15,541 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/config.json
[INFO|configuration_utils.py:769] 2024-08-13 21:52:15,541 >> Configuration saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-13 21:52:42,498 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-13 21:52:42,500 >> tokenizer config file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-13 21:52:42,500 >> Special tokens file saved in /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/special_tokens_map.json
[INFO|trainer.py:3788] 2024-08-13 21:52:43,475 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-13 21:52:43,475 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-13 21:52:43,475 >>   Batch size = 1
***** train metrics *****
  epoch                    =    14.5455
  total_flos               =     4502GF
  train_loss               =     0.5602
  train_runtime            = 0:41:36.43
  train_samples_per_second =      5.408
  train_steps_per_second   =      0.012
Figure saved at: /model/output/Llama-2-7b-hf-cluster_pair_score_WHATMAKESGOOD_1k_sharegpt-e15lr1e-5/training_loss.png
08/13/2024 21:52:43 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:04<00:00,  3.09it/s]
[INFO|modelcard.py:449] 2024-08-13 21:52:48,760 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =    14.5455
  eval_loss               =     1.5646
  eval_runtime            = 0:00:05.28
  eval_samples_per_second =     18.929
  eval_steps_per_second   =      2.839