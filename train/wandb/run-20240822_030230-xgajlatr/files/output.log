  0%|                                                                            | 0/45 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  2%|█▌                                                                  | 1/45 [00:28<21:02, 28.70s/it]
{'loss': 8.6107, 'grad_norm': 410.581885103325, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.28}

  4%|███                                                                 | 2/45 [00:50<17:37, 24.58s/it]

  7%|████▌                                                               | 3/45 [01:11<16:08, 23.05s/it]

  9%|██████                                                              | 4/45 [01:32<15:12, 22.26s/it]


 13%|█████████                                                           | 6/45 [02:14<14:01, 21.58s/it]
{'loss': 7.0446, 'grad_norm': 83.97326525355406, 'learning_rate': 8.666666666666668e-06, 'epoch': 1.68}

 16%|██████████▌                                                         | 7/45 [02:35<13:32, 21.38s/it]

 18%|████████████                                                        | 8/45 [02:56<13:06, 21.26s/it]

 20%|█████████████▌                                                      | 9/45 [03:17<12:42, 21.17s/it]

 22%|██████████████▉                                                    | 10/45 [03:38<12:18, 21.11s/it]

 24%|████████████████▍                                                  | 11/45 [03:59<11:56, 21.06s/it]

 27%|█████████████████▊                                                 | 12/45 [04:20<11:33, 21.02s/it]

 29%|███████████████████▎                                               | 13/45 [04:41<11:13, 21.03s/it]

 31%|████████████████████▊                                              | 14/45 [05:02<10:50, 21.00s/it]

 33%|██████████████████████▎                                            | 15/45 [05:23<10:29, 20.98s/it]

 36%|███████████████████████▊                                           | 16/45 [05:44<10:08, 20.98s/it]

 38%|█████████████████████████▎                                         | 17/45 [06:05<09:46, 20.96s/it]


 42%|████████████████████████████▎                                      | 19/45 [06:47<09:04, 20.93s/it]
{'loss': 4.5047, 'grad_norm': 73.59203487916508, 'learning_rate': 5.777777777777778e-06, 'epoch': 5.33}


 47%|███████████████████████████████▎                                   | 21/45 [07:29<08:22, 20.93s/it]
{'loss': 4.4216, 'grad_norm': 65.67710728918411, 'learning_rate': 5.333333333333334e-06, 'epoch': 5.89}

 49%|████████████████████████████████▊                                  | 22/45 [07:50<08:02, 20.96s/it]


 53%|███████████████████████████████████▋                               | 24/45 [08:32<07:20, 20.96s/it]
{'loss': 4.045, 'grad_norm': 53.5956541206934, 'learning_rate': 4.666666666666667e-06, 'epoch': 6.74}


 58%|██████████████████████████████████████▋                            | 26/45 [09:13<06:37, 20.93s/it]
{'loss': 3.7681, 'grad_norm': 54.72811438288597, 'learning_rate': 4.222222222222223e-06, 'epoch': 7.3}


 62%|█████████████████████████████████████████▋                         | 28/45 [09:55<05:56, 20.95s/it]
{'loss': 3.6537, 'grad_norm': 60.111832221675584, 'learning_rate': 3.777777777777778e-06, 'epoch': 7.86}


 67%|████████████████████████████████████████████▋                      | 30/45 [10:37<05:14, 20.94s/it]
{'loss': 3.851, 'grad_norm': 62.24808410392544, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.42}


 71%|███████████████████████████████████████████████▋                   | 32/45 [11:19<04:33, 21.01s/it]
{'loss': 3.3042, 'grad_norm': 53.58652921473129, 'learning_rate': 2.888888888888889e-06, 'epoch': 8.98}


 76%|██████████████████████████████████████████████████▌                | 34/45 [12:01<03:51, 21.00s/it]
{'loss': 3.2228, 'grad_norm': 50.826148420682536, 'learning_rate': 2.4444444444444447e-06, 'epoch': 9.54}


 80%|█████████████████████████████████████████████████████▌             | 36/45 [12:44<03:09, 21.07s/it]
{'loss': 3.2843, 'grad_norm': 47.63309375942056, 'learning_rate': 2.0000000000000003e-06, 'epoch': 10.11}


 84%|████████████████████████████████████████████████████████▌          | 38/45 [13:26<02:27, 21.09s/it]
{'loss': 2.9533, 'grad_norm': 44.54796287254615, 'learning_rate': 1.5555555555555558e-06, 'epoch': 10.67}

 87%|██████████████████████████████████████████████████████████         | 39/45 [13:47<02:06, 21.09s/it]


 91%|█████████████████████████████████████████████████████████████      | 41/45 [14:29<01:24, 21.09s/it]
{'loss': 2.9042, 'grad_norm': 45.51097427321102, 'learning_rate': 8.88888888888889e-07, 'epoch': 11.51}


 96%|████████████████████████████████████████████████████████████████   | 43/45 [15:11<00:42, 21.08s/it]
{'loss': 2.7888, 'grad_norm': 50.93583202790546, 'learning_rate': 4.444444444444445e-07, 'epoch': 12.07}

100%|███████████████████████████████████████████████████████████████████| 45/45 [15:53<00:00, 21.05s/it][INFO|trainer.py:2383] 2024-08-22 03:18:31,253 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████| 45/45 [15:53<00:00, 21.20s/it]
{'loss': 2.8539, 'grad_norm': 41.62283713646948, 'learning_rate': 0.0, 'epoch': 12.63}
{'train_runtime': 962.7085, 'train_samples_per_second': 14.023, 'train_steps_per_second': 0.047, 'train_loss': 4.689037375979954, 'epoch': 12.63}
[INFO|trainer.py:3478] 2024-08-22 03:18:38,933 >> Saving model checkpoint to /model/output/mistral-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-05
[INFO|configuration_utils.py:472] 2024-08-22 03:18:38,936 >> Configuration saved in /model/output/mistral-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-05/config.json
[INFO|configuration_utils.py:769] 2024-08-22 03:18:38,936 >> Configuration saved in /model/output/mistral-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-05/generation_config.json
***** train metrics *****
  epoch                    =    12.6316
  total_flos               =     1893GF
  train_loss               =      4.689
  train_runtime            = 0:16:02.70
  train_samples_per_second =     14.023
  train_steps_per_second   =      0.047
Figure saved at: /model/output/mistral-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-05/training_loss.png
08/22/2024 03:18:54 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
[INFO|modeling_utils.py:2698] 2024-08-22 03:18:53,910 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/mistral-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-05/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-22 03:18:53,911 >> tokenizer config file saved in /model/output/mistral-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-05/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-22 03:18:53,911 >> Special tokens file saved in /model/output/mistral-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-05/special_tokens_map.json
[INFO|trainer.py:3788] 2024-08-22 03:18:54,648 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-22 03:18:54,648 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-22 03:18:54,648 >>   Batch size = 1

100%|███████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  3.11it/s]
[INFO|modelcard.py:449] 2024-08-22 03:18:59,268 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =    12.6316
  eval_loss               =     2.6225
  eval_runtime            = 0:00:04.57
  eval_samples_per_second =     21.874
  eval_steps_per_second   =      2.844