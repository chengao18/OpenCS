  0%|                                                                            | 0/45 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  2%|█▌                                                                  | 1/45 [00:26<19:21, 26.39s/it]
{'loss': 6.7398, 'grad_norm': 718.6979328967074, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.28}

  4%|███                                                                 | 2/45 [00:47<16:52, 23.55s/it]


  9%|██████                                                              | 4/45 [01:30<15:08, 22.16s/it]
{'loss': 3.5396, 'grad_norm': 195.12854336949272, 'learning_rate': 9.111111111111112e-06, 'epoch': 1.12}

 11%|███████▌                                                            | 5/45 [01:52<14:35, 21.89s/it]

 13%|█████████                                                           | 6/45 [02:14<14:15, 21.94s/it]

 16%|██████████▌                                                         | 7/45 [02:36<13:51, 21.89s/it]

 18%|████████████                                                        | 8/45 [02:57<13:28, 21.84s/it]


 22%|██████████████▉                                                    | 10/45 [03:41<12:43, 21.83s/it]

 24%|████████████████▍                                                  | 11/45 [04:03<12:19, 21.76s/it]

 27%|█████████████████▊                                                 | 12/45 [04:24<11:57, 21.74s/it]
{'loss': 1.5884, 'grad_norm': 17.00233084463394, 'learning_rate': 7.333333333333333e-06, 'epoch': 3.37}

 29%|███████████████████▎                                               | 13/45 [04:46<11:34, 21.72s/it]


 33%|██████████████████████▎                                            | 15/45 [05:30<10:55, 21.86s/it]
{'loss': 1.1712, 'grad_norm': 10.26354494896542, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.21}

 36%|███████████████████████▊                                           | 16/45 [05:52<10:34, 21.89s/it]

 38%|█████████████████████████▎                                         | 17/45 [06:14<10:12, 21.89s/it]

 40%|██████████████████████████▊                                        | 18/45 [06:36<09:51, 21.92s/it]


 44%|█████████████████████████████▊                                     | 20/45 [07:20<09:12, 22.11s/it]

 47%|███████████████████████████████▎                                   | 21/45 [07:43<08:51, 22.16s/it]

 49%|████████████████████████████████▊                                  | 22/45 [08:05<08:30, 22.19s/it]

 51%|██████████████████████████████████▏                                | 23/45 [08:27<08:07, 22.17s/it]
{'loss': 0.5773, 'grad_norm': 10.082122814142522, 'learning_rate': 4.888888888888889e-06, 'epoch': 6.46}

 53%|███████████████████████████████████▋                               | 24/45 [08:50<07:47, 22.25s/it]

 56%|█████████████████████████████████████▏                             | 25/45 [09:12<07:25, 22.26s/it]

 58%|██████████████████████████████████████▋                            | 26/45 [09:34<07:02, 22.23s/it]


 62%|█████████████████████████████████████████▋                         | 28/45 [10:19<06:18, 22.29s/it]

 64%|███████████████████████████████████████████▏                       | 29/45 [10:41<05:57, 22.33s/it]

 67%|████████████████████████████████████████████▋                      | 30/45 [11:03<05:34, 22.32s/it]
{'loss': 0.1985, 'grad_norm': 7.15152127814753, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.42}

 69%|██████████████████████████████████████████████▏                    | 31/45 [11:26<05:12, 22.31s/it]

 71%|███████████████████████████████████████████████▋                   | 32/45 [11:48<04:50, 22.31s/it]

 73%|█████████████████████████████████████████████████▏                 | 33/45 [12:10<04:27, 22.29s/it]


 78%|████████████████████████████████████████████████████               | 35/45 [12:55<03:42, 22.29s/it]

 80%|█████████████████████████████████████████████████████▌             | 36/45 [13:17<03:20, 22.26s/it]

 82%|███████████████████████████████████████████████████████            | 37/45 [13:39<02:58, 22.29s/it]

 84%|████████████████████████████████████████████████████████▌          | 38/45 [14:01<02:35, 22.23s/it]

 87%|██████████████████████████████████████████████████████████         | 39/45 [14:24<02:13, 22.24s/it]
{'loss': 0.0635, 'grad_norm': 4.815093493479867, 'learning_rate': 1.3333333333333334e-06, 'epoch': 10.95}

 89%|███████████████████████████████████████████████████████████▌       | 40/45 [14:46<01:51, 22.23s/it]

 91%|█████████████████████████████████████████████████████████████      | 41/45 [15:08<01:28, 22.22s/it]

 93%|██████████████████████████████████████████████████████████████▌    | 42/45 [15:30<01:06, 22.24s/it]

 96%|████████████████████████████████████████████████████████████████   | 43/45 [15:53<00:44, 22.23s/it]

100%|███████████████████████████████████████████████████████████████████| 45/45 [16:37<00:00, 22.23s/it][INFO|trainer.py:2383] 2024-08-22 03:48:40,713 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████| 45/45 [16:37<00:00, 22.17s/it]
{'loss': 0.0452, 'grad_norm': 4.978811190164733, 'learning_rate': 0.0, 'epoch': 12.63}
{'train_runtime': 1005.3207, 'train_samples_per_second': 13.429, 'train_steps_per_second': 0.045, 'train_loss': 1.1959371102352938, 'epoch': 12.63}
[INFO|trainer.py:3478] 2024-08-22 03:48:48,366 >> Saving model checkpoint to /model/output/mistral-7b-refined_alpaca_1k_longest_beautified-e15lr1e-05
[INFO|configuration_utils.py:472] 2024-08-22 03:48:48,369 >> Configuration saved in /model/output/mistral-7b-refined_alpaca_1k_longest_beautified-e15lr1e-05/config.json
[INFO|configuration_utils.py:769] 2024-08-22 03:48:48,370 >> Configuration saved in /model/output/mistral-7b-refined_alpaca_1k_longest_beautified-e15lr1e-05/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-22 03:49:02,702 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/mistral-7b-refined_alpaca_1k_longest_beautified-e15lr1e-05/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-22 03:49:02,703 >> tokenizer config file saved in /model/output/mistral-7b-refined_alpaca_1k_longest_beautified-e15lr1e-05/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-22 03:49:02,703 >> Special tokens file saved in /model/output/mistral-7b-refined_alpaca_1k_longest_beautified-e15lr1e-05/special_tokens_map.json
[INFO|trainer.py:3788] 2024-08-22 03:49:03,380 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-22 03:49:03,380 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-22 03:49:03,380 >>   Batch size = 1
***** train metrics *****
  epoch                    =    12.6316
  total_flos               =     6816GF
  train_loss               =     1.1959
  train_runtime            = 0:16:45.32
  train_samples_per_second =     13.429
  train_steps_per_second   =      0.045
Figure saved at: /model/output/mistral-7b-refined_alpaca_1k_longest_beautified-e15lr1e-05/training_loss.png
08/22/2024 03:49:03 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.

 85%|████████████████████████████████████████████████████████▋          | 11/13 [00:03<00:00,  3.06it/s]
***** eval metrics *****
  epoch                   =    12.6316
  eval_loss               =     1.9577
  eval_runtime            = 0:00:04.57
  eval_samples_per_second =     21.843
100%|███████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  3.12it/s]
[INFO|modelcard.py:449] 2024-08-22 03:49:07,958 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}