  0%|                                                                             | 0/156 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|▍                                                                  | 1/156 [00:38<1:38:19, 38.06s/it]

  1%|▊                                                                  | 2/156 [01:10<1:29:24, 34.84s/it]
{'loss': 1.7795, 'grad_norm': 8.112555730831058, 'learning_rate': 8.510638297872341e-07, 'epoch': 0.04}

  2%|█▎                                                                 | 3/156 [01:43<1:26:19, 33.85s/it]


  3%|██▏                                                                | 5/156 [02:50<1:25:05, 33.81s/it]

  4%|██▌                                                                | 6/156 [03:21<1:22:31, 33.01s/it]

  4%|███                                                                | 7/156 [03:56<1:23:32, 33.64s/it]

  5%|███▍                                                               | 8/156 [04:28<1:21:28, 33.03s/it]

  6%|███▊                                                               | 9/156 [05:00<1:20:11, 32.73s/it]

  6%|████▏                                                             | 10/156 [05:32<1:18:45, 32.37s/it]

  7%|████▋                                                             | 11/156 [06:04<1:18:23, 32.44s/it]

  8%|█████                                                             | 12/156 [06:39<1:19:10, 32.99s/it]

  8%|█████▌                                                            | 13/156 [07:11<1:18:07, 32.78s/it]

  9%|█████▉                                                            | 14/156 [07:43<1:16:53, 32.49s/it]

 10%|██████▎                                                           | 15/156 [08:14<1:15:50, 32.27s/it]

 10%|██████▊                                                           | 16/156 [08:48<1:16:07, 32.62s/it]
{'loss': 1.5466, 'grad_norm': 1.4084282006057662, 'learning_rate': 6.808510638297873e-06, 'epoch': 0.31}


 12%|███████▌                                                          | 18/156 [09:59<1:18:14, 34.02s/it]

 12%|████████                                                          | 19/156 [10:34<1:18:47, 34.51s/it]

 13%|████████▍                                                         | 20/156 [11:10<1:19:03, 34.88s/it]

 13%|████████▉                                                         | 21/156 [11:44<1:18:14, 34.78s/it]

 14%|█████████▎                                                        | 22/156 [12:20<1:18:14, 35.03s/it]

 15%|█████████▋                                                        | 23/156 [12:55<1:17:17, 34.86s/it]

 15%|██████████▏                                                       | 24/156 [13:28<1:15:56, 34.52s/it]

 16%|██████████▌                                                       | 25/156 [14:03<1:15:34, 34.62s/it]

 17%|███████████                                                       | 26/156 [14:38<1:15:06, 34.66s/it]

 17%|███████████▍                                                      | 27/156 [15:12<1:14:10, 34.50s/it]

 18%|███████████▊                                                      | 28/156 [15:47<1:13:59, 34.69s/it]

 19%|████████████▎                                                     | 29/156 [16:21<1:12:49, 34.40s/it]

 19%|████████████▋                                                     | 30/156 [16:55<1:11:57, 34.27s/it]

 20%|█████████████                                                     | 31/156 [17:29<1:11:24, 34.28s/it]
{'loss': 1.3695, 'grad_norm': 0.8726378982786308, 'learning_rate': 1.3191489361702127e-05, 'epoch': 0.59}


 21%|█████████████▉                                                    | 33/156 [18:38<1:10:31, 34.40s/it]

 22%|██████████████▍                                                   | 34/156 [19:13<1:10:22, 34.61s/it]

 22%|██████████████▊                                                   | 35/156 [19:48<1:09:38, 34.53s/it]

 23%|███████████████▏                                                  | 36/156 [20:22<1:09:16, 34.64s/it]

 24%|███████████████▋                                                  | 37/156 [20:57<1:08:29, 34.53s/it]

 24%|████████████████                                                  | 38/156 [21:30<1:07:22, 34.26s/it]

 25%|████████████████▌                                                 | 39/156 [22:05<1:07:01, 34.37s/it]

 26%|████████████████▉                                                 | 40/156 [22:41<1:07:07, 34.72s/it]

 26%|█████████████████▎                                                | 41/156 [23:17<1:07:27, 35.20s/it]

 27%|█████████████████▊                                                | 42/156 [23:53<1:07:13, 35.38s/it]

 28%|██████████████████▏                                               | 43/156 [24:28<1:06:50, 35.49s/it]

 28%|██████████████████▌                                               | 44/156 [25:03<1:05:43, 35.21s/it]

 29%|███████████████████                                               | 45/156 [25:37<1:04:41, 34.97s/it]
{'loss': 1.3843, 'grad_norm': 0.8080466810895799, 'learning_rate': 1.914893617021277e-05, 'epoch': 0.86}


 30%|███████████████████▉                                              | 47/156 [26:48<1:03:38, 35.03s/it]
{'loss': 1.3935, 'grad_norm': 0.8753331776212315, 'learning_rate': 2e-05, 'epoch': 0.9}


 31%|████████████████████▋                                             | 49/156 [27:59<1:02:51, 35.25s/it]

 32%|█████████████████████▏                                            | 50/156 [28:33<1:01:36, 34.87s/it]

 33%|█████████████████████▌                                            | 51/156 [29:08<1:00:57, 34.83s/it]

 33%|██████████████████████                                            | 52/156 [29:42<1:00:08, 34.70s/it]

 34%|███████████████████████                                             | 53/156 [30:17<59:38, 34.74s/it]

 35%|███████████████████████▌                                            | 54/156 [30:51<58:59, 34.71s/it]
{'loss': 1.222, 'grad_norm': 0.9485606671318307, 'learning_rate': 1.9797166732215078e-05, 'epoch': 1.03}


 36%|████████████████████████▍                                           | 56/156 [32:01<57:50, 34.71s/it]

 37%|████████████████████████▊                                           | 57/156 [32:35<56:55, 34.49s/it]

 37%|█████████████████████████▎                                          | 58/156 [33:10<56:44, 34.74s/it]

 38%|█████████████████████████▋                                          | 59/156 [33:46<56:36, 35.01s/it]

 38%|██████████████████████████▏                                         | 60/156 [34:20<55:24, 34.64s/it]

 39%|██████████████████████████▌                                         | 61/156 [34:56<55:27, 35.03s/it]

 40%|███████████████████████████                                         | 62/156 [35:30<54:25, 34.74s/it]

 40%|███████████████████████████▍                                        | 63/156 [36:04<53:43, 34.66s/it]
{'loss': 1.1289, 'grad_norm': 0.8624864950366677, 'learning_rate': 1.8955408240595396e-05, 'epoch': 1.21}


 42%|████████████████████████████▎                                       | 65/156 [37:16<53:52, 35.52s/it]

 42%|████████████████████████████▊                                       | 66/156 [37:51<52:58, 35.32s/it]
{'loss': 1.1112, 'grad_norm': 0.8394171052485542, 'learning_rate': 1.8537676285033886e-05, 'epoch': 1.26}


 44%|█████████████████████████████▋                                      | 68/156 [39:01<51:15, 34.95s/it]

 44%|██████████████████████████████                                      | 69/156 [39:36<50:52, 35.08s/it]

 45%|██████████████████████████████▌                                     | 70/156 [40:10<49:56, 34.84s/it]

 46%|██████████████████████████████▉                                     | 71/156 [40:46<49:35, 35.00s/it]

 46%|███████████████████████████████▍                                    | 72/156 [41:20<48:52, 34.92s/it]

 47%|███████████████████████████████▊                                    | 73/156 [41:54<47:44, 34.51s/it]

 47%|████████████████████████████████▎                                   | 74/156 [42:30<47:38, 34.86s/it]
{'loss': 1.1042, 'grad_norm': 0.8240891847423628, 'learning_rate': 1.712183430261319e-05, 'epoch': 1.42}


 49%|█████████████████████████████████▏                                  | 76/156 [43:40<46:43, 35.04s/it]

 49%|█████████████████████████████████▌                                  | 77/156 [44:14<45:41, 34.71s/it]

 50%|██████████████████████████████████                                  | 78/156 [44:48<44:56, 34.57s/it]

 51%|██████████████████████████████████▍                                 | 79/156 [45:22<44:02, 34.32s/it]

 51%|██████████████████████████████████▊                                 | 80/156 [45:56<43:25, 34.28s/it]

 52%|███████████████████████████████████▎                                | 81/156 [46:30<42:39, 34.12s/it]

 53%|███████████████████████████████████▋                                | 82/156 [47:05<42:18, 34.31s/it]

 53%|████████████████████████████████████▏                               | 83/156 [47:39<41:37, 34.21s/it]

 54%|████████████████████████████████████▌                               | 84/156 [48:13<40:56, 34.12s/it]

 54%|█████████████████████████████████████                               | 85/156 [48:47<40:16, 34.04s/it]

 55%|█████████████████████████████████████▍                              | 86/156 [49:20<39:31, 33.88s/it]

 56%|█████████████████████████████████████▉                              | 87/156 [49:53<38:36, 33.57s/it]

 56%|██████████████████████████████████████▎                             | 88/156 [50:27<38:12, 33.71s/it]
{'loss': 1.1053, 'grad_norm': 0.8071561925283175, 'learning_rate': 1.3793524546768358e-05, 'epoch': 1.68}


 58%|███████████████████████████████████████▏                            | 90/156 [51:37<37:51, 34.42s/it]

 58%|███████████████████████████████████████▋                            | 91/156 [52:11<37:14, 34.37s/it]
{'loss': 1.1426, 'grad_norm': 0.7927528049728539, 'learning_rate': 1.2980321796293838e-05, 'epoch': 1.74}


 60%|████████████████████████████████████████▌                           | 93/156 [53:20<36:04, 34.36s/it]

 60%|████████████████████████████████████████▉                           | 94/156 [53:53<35:11, 34.06s/it]
{'loss': 1.1301, 'grad_norm': 0.855519785294974, 'learning_rate': 1.2144851014515055e-05, 'epoch': 1.8}

 61%|█████████████████████████████████████████▍                          | 95/156 [54:28<34:45, 34.19s/it]


 62%|██████████████████████████████████████████▎                         | 97/156 [55:36<33:27, 34.03s/it]

 63%|██████████████████████████████████████████▋                         | 98/156 [56:11<33:14, 34.39s/it]
{'loss': 1.1515, 'grad_norm': 0.8471272659417803, 'learning_rate': 1.1007058259945584e-05, 'epoch': 1.88}


 64%|██████████████████████████████████████████▉                        | 100/156 [57:19<32:03, 34.35s/it]

 65%|███████████████████████████████████████████▍                       | 101/156 [57:52<31:09, 34.00s/it]

 65%|███████████████████████████████████████████▊                       | 102/156 [58:27<30:51, 34.28s/it]

 66%|████████████████████████████████████████████▏                      | 103/156 [59:01<30:07, 34.10s/it]

 67%|████████████████████████████████████████████▋                      | 104/156 [59:35<29:39, 34.23s/it]

 67%|███████████████████████████████████████████▊                     | 105/156 [1:00:08<28:47, 33.87s/it]

 68%|████████████████████████████████████████████▏                    | 106/156 [1:00:43<28:20, 34.02s/it]

 69%|████████████████████████████████████████████▌                    | 107/156 [1:01:17<27:43, 33.96s/it]

 69%|█████████████████████████████████████████████                    | 108/156 [1:01:50<26:55, 33.65s/it]

 70%|█████████████████████████████████████████████▍                   | 109/156 [1:02:23<26:13, 33.47s/it]

 71%|█████████████████████████████████████████████▊                   | 110/156 [1:02:56<25:37, 33.42s/it]
{'loss': 0.8669, 'grad_norm': 1.2742612549821921, 'learning_rate': 7.574566914899779e-06, 'epoch': 2.11}


 72%|██████████████████████████████████████████████▋                  | 112/156 [1:04:04<24:41, 33.67s/it]

 72%|███████████████████████████████████████████████                  | 113/156 [1:04:37<24:01, 33.52s/it]

 73%|███████████████████████████████████████████████▌                 | 114/156 [1:05:12<23:43, 33.90s/it]

 74%|███████████████████████████████████████████████▉                 | 115/156 [1:05:48<23:32, 34.46s/it]
{'loss': 0.8549, 'grad_norm': 1.023137347054243, 'learning_rate': 6.206475453231644e-06, 'epoch': 2.2}


 75%|████████████████████████████████████████████████▊                | 117/156 [1:06:55<22:11, 34.14s/it]
{'loss': 0.8295, 'grad_norm': 0.9109024787046617, 'learning_rate': 5.6797201335722064e-06, 'epoch': 2.24}

 76%|█████████████████████████████████████████████████▏               | 118/156 [1:07:29<21:25, 33.83s/it]


 77%|██████████████████████████████████████████████████               | 120/156 [1:08:37<20:30, 34.18s/it]
{'loss': 0.8577, 'grad_norm': 1.0653069387857534, 'learning_rate': 4.917030223798057e-06, 'epoch': 2.3}


 78%|██████████████████████████████████████████████████▊              | 122/156 [1:09:44<19:07, 33.76s/it]
{'loss': 0.8569, 'grad_norm': 0.9634765328052144, 'learning_rate': 4.429328831625565e-06, 'epoch': 2.33}


 79%|███████████████████████████████████████████████████▋             | 124/156 [1:10:50<17:47, 33.34s/it]

 80%|████████████████████████████████████████████████████             | 125/156 [1:11:23<17:15, 33.40s/it]

 81%|████████████████████████████████████████████████████▌            | 126/156 [1:11:56<16:35, 33.18s/it]
{'loss': 0.8642, 'grad_norm': 0.9230286120570667, 'learning_rate': 3.511000298021098e-06, 'epoch': 2.41}

 81%|████████████████████████████████████████████████████▉            | 127/156 [1:12:29<15:59, 33.07s/it]


 83%|█████████████████████████████████████████████████████▊           | 129/156 [1:13:34<14:45, 32.79s/it]

 83%|██████████████████████████████████████████████████████▏          | 130/156 [1:14:06<14:07, 32.60s/it]
{'loss': 0.8233, 'grad_norm': 0.9053231136185756, 'learning_rate': 2.678823375955314e-06, 'epoch': 2.49}


 85%|███████████████████████████████████████████████████████          | 132/156 [1:15:14<13:21, 33.38s/it]

 85%|███████████████████████████████████████████████████████▍         | 133/156 [1:15:47<12:44, 33.23s/it]

 86%|███████████████████████████████████████████████████████▊         | 134/156 [1:16:21<12:11, 33.26s/it]

 87%|████████████████████████████████████████████████████████▎        | 135/156 [1:16:54<11:37, 33.20s/it]

 87%|████████████████████████████████████████████████████████▋        | 136/156 [1:17:26<10:57, 32.87s/it]

 88%|█████████████████████████████████████████████████████████        | 137/156 [1:18:00<10:29, 33.15s/it]

 88%|█████████████████████████████████████████████████████████▌       | 138/156 [1:18:35<10:11, 33.96s/it]

 89%|█████████████████████████████████████████████████████████▉       | 139/156 [1:19:08<09:32, 33.66s/it]

 90%|██████████████████████████████████████████████████████████▎      | 140/156 [1:19:41<08:52, 33.26s/it]

 90%|██████████████████████████████████████████████████████████▊      | 141/156 [1:20:14<08:18, 33.20s/it]
{'loss': 0.8295, 'grad_norm': 0.853074891851073, 'learning_rate': 9.200771748932513e-07, 'epoch': 2.7}


 92%|███████████████████████████████████████████████████████████▌     | 143/156 [1:21:18<07:03, 32.58s/it]

 92%|████████████████████████████████████████████████████████████     | 144/156 [1:21:50<06:31, 32.66s/it]

 93%|████████████████████████████████████████████████████████████▍    | 145/156 [1:22:22<05:55, 32.32s/it]

 94%|████████████████████████████████████████████████████████████▊    | 146/156 [1:22:55<05:24, 32.40s/it]
{'loss': 0.8413, 'grad_norm': 0.8591563900414061, 'learning_rate': 4.124850842338779e-07, 'epoch': 2.79}


 95%|█████████████████████████████████████████████████████████████▋   | 148/156 [1:24:01<04:21, 32.74s/it]

 96%|██████████████████████████████████████████████████████████████   | 149/156 [1:24:34<03:50, 32.92s/it]
{'loss': 0.8402, 'grad_norm': 0.8665246578170643, 'learning_rate': 2.028332677849254e-07, 'epoch': 2.85}


 97%|██████████████████████████████████████████████████████████████▉  | 151/156 [1:25:40<02:45, 33.07s/it]

 97%|███████████████████████████████████████████████████████████████▎ | 152/156 [1:26:12<02:10, 32.66s/it]

 98%|███████████████████████████████████████████████████████████████▊ | 153/156 [1:26:45<01:37, 32.63s/it]

 99%|████████████████████████████████████████████████████████████████▏| 154/156 [1:27:18<01:05, 32.80s/it]

 99%|████████████████████████████████████████████████████████████████▌| 155/156 [1:27:51<00:33, 33.02s/it]
100%|█████████████████████████████████████████████████████████████████| 156/156 [1:28:23<00:00, 32.71s/it][INFO|trainer.py:2383] 2024-08-21 15:58:34,216 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|█████████████████████████████████████████████████████████████████| 156/156 [1:28:23<00:00, 34.00s/it]
{'loss': 0.8169, 'grad_norm': 0.8806476550034248, 'learning_rate': 0.0, 'epoch': 2.99}
{'train_runtime': 5312.2119, 'train_samples_per_second': 26.43, 'train_steps_per_second': 0.029, 'train_loss': 1.1455845901599297, 'epoch': 2.99}
[INFO|trainer.py:3478] 2024-08-21 15:58:42,156 >> Saving model checkpoint to /model/output/llama2-7b-fastchat_alpaca_52k_beautified-e3lr2e-5
[INFO|configuration_utils.py:472] 2024-08-21 15:58:42,159 >> Configuration saved in /model/output/llama2-7b-fastchat_alpaca_52k_beautified-e3lr2e-5/config.json
[INFO|configuration_utils.py:769] 2024-08-21 15:58:42,160 >> Configuration saved in /model/output/llama2-7b-fastchat_alpaca_52k_beautified-e3lr2e-5/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-21 15:58:56,400 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/llama2-7b-fastchat_alpaca_52k_beautified-e3lr2e-5/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-21 15:58:56,401 >> tokenizer config file saved in /model/output/llama2-7b-fastchat_alpaca_52k_beautified-e3lr2e-5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-21 15:58:56,402 >> Special tokens file saved in /model/output/llama2-7b-fastchat_alpaca_52k_beautified-e3lr2e-5/special_tokens_map.json
[INFO|trainer.py:3788] 2024-08-21 15:58:57,151 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-21 15:58:57,152 >>   Num examples = 5200
[INFO|trainer.py:3793] 2024-08-21 15:58:57,152 >>   Batch size = 4
  1%|▋                                                                    | 2/186 [00:00<01:00,  3.04it/s]
***** train metrics *****
  epoch                    =     2.9856
  total_flos               =    45797GF
  train_loss               =     1.1456
  train_runtime            = 1:28:32.21
  train_samples_per_second =      26.43
  train_steps_per_second   =      0.029
Figure saved at: /model/output/llama2-7b-fastchat_alpaca_52k_beautified-e3lr2e-5/training_loss.png




































100%|███████████████████████████████████████████████████████████████████| 186/186 [01:14<00:00,  2.49it/s]
[INFO|modelcard.py:449] 2024-08-21 16:00:12,271 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =     2.9856
  eval_loss               =     1.4629
  eval_runtime            = 0:01:15.11
  eval_samples_per_second =     69.226
  eval_steps_per_second   =      2.476