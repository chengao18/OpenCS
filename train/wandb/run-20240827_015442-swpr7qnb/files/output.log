  0%|                                                                                        | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  7%|█████▎                                                                          | 1/15 [00:43<10:15, 43.94s/it]
{'loss': 4.1557, 'grad_norm': 740.3310492610722, 'learning_rate': 1.8666666666666667e-06, 'epoch': 0.56}


 20%|████████████████                                                                | 3/15 [02:02<08:04, 40.34s/it]
{'loss': 2.2997, 'grad_norm': 218.08067242995193, 'learning_rate': 1.6e-06, 'epoch': 1.68}


 33%|███████████████████████████████▋                                                               | 5/15 [03:21<06:37, 39.75s/it]

 40%|██████████████████████████████████████                                                         | 6/15 [04:00<05:56, 39.62s/it]
{'loss': 1.674, 'grad_norm': 66.0093136775978, 'learning_rate': 1.2e-06, 'epoch': 3.37}


 53%|██████████████████████████████████████████████████▋                                            | 8/15 [05:19<04:36, 39.50s/it]

 60%|█████████████████████████████████████████████████████████                                      | 9/15 [05:58<03:56, 39.49s/it]

 67%|██████████████████████████████████████████████████████████████▋                               | 10/15 [06:38<03:17, 39.49s/it]

 73%|████████████████████████████████████████████████████████████████████▉                         | 11/15 [07:17<02:37, 39.48s/it]

 80%|███████████████████████████████████████████████████████████████████████████▏                  | 12/15 [07:57<01:58, 39.48s/it]

 87%|█████████████████████████████████████████████████████████████████████████████████▍            | 13/15 [08:36<01:18, 39.41s/it]

 93%|███████████████████████████████████████████████████████████████████████████████████████▋      | 14/15 [09:16<00:39, 39.43s/it]
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [09:55<00:00, 39.40s/it][INFO|trainer.py:2383] 2024-08-27 02:04:45,128 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [09:55<00:00, 39.69s/it]
{'loss': 1.361, 'grad_norm': 21.443942778434323, 'learning_rate': 0.0, 'epoch': 8.42}
{'train_runtime': 604.1524, 'train_samples_per_second': 22.345, 'train_steps_per_second': 0.025, 'train_loss': 1.851007604598999, 'epoch': 8.42}
[INFO|trainer.py:3478] 2024-08-27 02:04:52,729 >> Saving model checkpoint to /model/output/mistral-7b-filtered_evol_instruct_1k_score_beautified-e15lr2e-06
[INFO|configuration_utils.py:472] 2024-08-27 02:04:52,732 >> Configuration saved in /model/output/mistral-7b-filtered_evol_instruct_1k_score_beautified-e15lr2e-06/config.json
[INFO|configuration_utils.py:769] 2024-08-27 02:04:52,732 >> Configuration saved in /model/output/mistral-7b-filtered_evol_instruct_1k_score_beautified-e15lr2e-06/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-27 02:05:07,440 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/mistral-7b-filtered_evol_instruct_1k_score_beautified-e15lr2e-06/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-27 02:05:07,441 >> tokenizer config file saved in /model/output/mistral-7b-filtered_evol_instruct_1k_score_beautified-e15lr2e-06/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-27 02:05:07,441 >> Special tokens file saved in /model/output/mistral-7b-filtered_evol_instruct_1k_score_beautified-e15lr2e-06/special_tokens_map.json
***** train metrics *****
  epoch                    =     8.4211
  total_flos               =     7300GF
  train_loss               =      1.851
  train_runtime            = 0:10:04.15
  train_samples_per_second =     22.345
  train_steps_per_second   =      0.025
Figure saved at: /model/output/mistral-7b-filtered_evol_instruct_1k_score_beautified-e15lr2e-06/training_loss.png
08/27/2024 02:05:08 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
[INFO|trainer.py:3788] 2024-08-27 02:05:08,110 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-27 02:05:08,111 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-27 02:05:08,111 >>   Batch size = 1

100%|██████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  3.12it/s]
[INFO|modelcard.py:449] 2024-08-27 02:05:12,666 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =     8.4211
  eval_loss               =     1.2091
  eval_runtime            = 0:00:04.55
  eval_samples_per_second =     21.952
  eval_steps_per_second   =      2.854