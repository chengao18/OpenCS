  0%|                                                                                                                                                        | 0/45 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  2%|███▏                                                                                                                                            | 1/45 [00:25<18:58, 25.88s/it]
{'loss': 8.2799, 'grad_norm': 583.4815764015176, 'learning_rate': 1.9555555555555556e-06, 'epoch': 0.28}


  7%|█████████▌                                                                                                                                      | 3/45 [01:07<15:29, 22.13s/it]
{'loss': 5.4643, 'grad_norm': 217.75265497807368, 'learning_rate': 1.8666666666666667e-06, 'epoch': 0.84}

  9%|████████████▊                                                                                                                                   | 4/45 [01:28<14:44, 21.58s/it]


 13%|███████████████████▏                                                                                                                            | 6/45 [02:10<13:41, 21.06s/it]
{'loss': 4.1869, 'grad_norm': 171.22250577647495, 'learning_rate': 1.7333333333333334e-06, 'epoch': 1.68}

 16%|██████████████████████▍                                                                                                                         | 7/45 [02:30<13:15, 20.93s/it]

 18%|█████████████████████████▌                                                                                                                      | 8/45 [02:51<12:52, 20.87s/it]


 22%|███████████████████████████████▊                                                                                                               | 10/45 [03:32<12:04, 20.69s/it]
{'loss': 3.3231, 'grad_norm': 108.34123068169846, 'learning_rate': 1.5555555555555556e-06, 'epoch': 2.81}

 24%|██████████████████████████████████▉                                                                                                            | 11/45 [03:52<11:40, 20.60s/it]

 27%|██████████████████████████████████████▏                                                                                                        | 12/45 [04:13<11:20, 20.61s/it]


 31%|████████████████████████████████████████████▍                                                                                                  | 14/45 [04:54<10:37, 20.57s/it]

 33%|███████████████████████████████████████████████▋                                                                                               | 15/45 [05:14<10:14, 20.47s/it]
{'loss': 2.792, 'grad_norm': 62.37837580105886, 'learning_rate': 1.3333333333333332e-06, 'epoch': 4.21}

 36%|██████████████████████████████████████████████████▊                                                                                            | 16/45 [05:35<09:54, 20.50s/it]


 40%|█████████████████████████████████████████████████████████▏                                                                                     | 18/45 [06:16<09:13, 20.49s/it]

 42%|████████████████████████████████████████████████████████████▍                                                                                  | 19/45 [06:36<08:53, 20.52s/it]
{'loss': 2.3825, 'grad_norm': 56.80980982354632, 'learning_rate': 1.1555555555555554e-06, 'epoch': 5.33}

 44%|███████████████████████████████████████████████████████████████▌                                                                               | 20/45 [06:57<08:32, 20.51s/it]

 47%|██████████████████████████████████████████████████████████████████▋                                                                            | 21/45 [07:17<08:12, 20.51s/it]


 51%|█████████████████████████████████████████████████████████████████████████                                                                      | 23/45 [07:58<07:30, 20.47s/it]

 53%|████████████████████████████████████████████████████████████████████████████▎                                                                  | 24/45 [08:19<07:08, 20.41s/it]
{'loss': 2.4779, 'grad_norm': 48.76264108942034, 'learning_rate': 9.333333333333333e-07, 'epoch': 6.74}

 56%|███████████████████████████████████████████████████████████████████████████████▍                                                               | 25/45 [08:39<06:50, 20.52s/it]


 60%|█████████████████████████████████████████████████████████████████████████████████████▊                                                         | 27/45 [09:20<06:09, 20.51s/it]

 62%|████████████████████████████████████████████████████████████████████████████████████████▉                                                      | 28/45 [09:41<05:49, 20.56s/it]
{'loss': 2.1663, 'grad_norm': 45.751937074770396, 'learning_rate': 7.555555555555555e-07, 'epoch': 7.86}


 67%|███████████████████████████████████████████████████████████████████████████████████████████████▎                                               | 30/45 [10:22<05:09, 20.60s/it]

 69%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                                            | 31/45 [10:43<04:48, 20.62s/it]
{'loss': 2.1773, 'grad_norm': 37.6795004077358, 'learning_rate': 6.222222222222223e-07, 'epoch': 8.7}


 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 33/45 [11:24<04:07, 20.63s/it]

 76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                   | 34/45 [11:45<03:47, 20.67s/it]
{'loss': 2.127, 'grad_norm': 43.70347973130973, 'learning_rate': 4.888888888888889e-07, 'epoch': 9.54}


 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                            | 36/45 [12:26<03:05, 20.66s/it]

 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 37/45 [12:47<02:44, 20.62s/it]

 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                      | 38/45 [13:07<02:24, 20.62s/it]
{'loss': 2.0872, 'grad_norm': 41.27927021416515, 'learning_rate': 3.111111111111111e-07, 'epoch': 10.67}


 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                | 40/45 [13:49<01:42, 20.59s/it]

 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎            | 41/45 [14:09<01:22, 20.57s/it]
{'loss': 2.2587, 'grad_norm': 50.8518283571557, 'learning_rate': 1.7777777777777776e-07, 'epoch': 11.51}


 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋      | 43/45 [14:50<00:41, 20.58s/it]

 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 44/45 [15:11<00:20, 20.53s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [15:31<00:00, 20.56s/it][INFO|trainer.py:2383] 2024-08-28 04:54:06,856 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [15:31<00:00, 20.71s/it]
{'loss': 1.8554, 'grad_norm': 34.443901117886604, 'learning_rate': 0.0, 'epoch': 12.63}
{'train_runtime': 940.1654, 'train_samples_per_second': 14.359, 'train_steps_per_second': 0.048, 'train_loss': 2.914585190349155, 'epoch': 12.63}
[INFO|trainer.py:3478] 2024-08-28 04:54:14,382 >> Saving model checkpoint to /model/output/mistral-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr2e-06
[INFO|configuration_utils.py:472] 2024-08-28 04:54:14,386 >> Configuration saved in /model/output/mistral-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr2e-06/config.json
[INFO|configuration_utils.py:769] 2024-08-28 04:54:14,386 >> Configuration saved in /model/output/mistral-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr2e-06/generation_config.json
***** train metrics *****
[INFO|modeling_utils.py:2698] 2024-08-28 04:54:29,341 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/mistral-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr2e-06/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-28 04:54:29,342 >> tokenizer config file saved in /model/output/mistral-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr2e-06/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-28 04:54:29,342 >> Special tokens file saved in /model/output/mistral-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr2e-06/special_tokens_map.json
[INFO|trainer.py:3788] 2024-08-28 04:54:30,010 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-28 04:54:30,010 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-28 04:54:30,010 >>   Batch size = 1
 15%|██████████████████████▏                                                                                                                         | 2/13 [00:00<00:03,  3.39it/s]
  epoch                    =    12.6316
  total_flos               =     3987GF
  train_loss               =     2.9146
  train_runtime            = 0:15:40.16
  train_samples_per_second =     14.359
  train_steps_per_second   =      0.048
Figure saved at: /model/output/mistral-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr2e-06/training_loss.png

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  3.15it/s]
[INFO|modelcard.py:449] 2024-08-28 04:54:34,525 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =    12.6316
  eval_loss               =     1.1568
  eval_runtime            = 0:00:04.51
  eval_samples_per_second =     22.153
  eval_steps_per_second   =       2.88