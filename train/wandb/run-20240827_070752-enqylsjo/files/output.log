  0%|                                                                                                       | 0/45 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  2%|██                                                                                             | 1/45 [00:25<18:49, 25.68s/it]
{'loss': 1.1908, 'grad_norm': 4.073184707810392, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.28}


  7%|██████▎                                                                                        | 3/45 [01:06<15:12, 21.74s/it]

  9%|████████▍                                                                                      | 4/45 [01:27<14:34, 21.32s/it]
{'loss': 0.971, 'grad_norm': 1.183668883302705, 'learning_rate': 9.111111111111112e-06, 'epoch': 1.12}


 13%|████████████▋                                                                                  | 6/45 [02:09<13:37, 20.97s/it]

 16%|██████████████▊                                                                                | 7/45 [02:29<13:13, 20.89s/it]
{'loss': 0.9367, 'grad_norm': 1.375934673291694, 'learning_rate': 8.444444444444446e-06, 'epoch': 1.96}


 20%|███████████████████                                                                            | 9/45 [03:10<12:26, 20.73s/it]

 22%|████████████████████▉                                                                         | 10/45 [03:31<12:04, 20.70s/it]

 24%|██████████████████████▉                                                                       | 11/45 [03:52<11:42, 20.67s/it]

 27%|█████████████████████████                                                                     | 12/45 [04:12<11:22, 20.67s/it]

 29%|███████████████████████████▏                                                                  | 13/45 [04:33<11:00, 20.64s/it]

 31%|█████████████████████████████▏                                                                | 14/45 [04:53<10:38, 20.61s/it]

 33%|███████████████████████████████▎                                                              | 15/45 [05:14<10:17, 20.58s/it]

 36%|█████████████████████████████████▍                                                            | 16/45 [05:35<09:56, 20.58s/it]

 38%|███████████████████████████████████▌                                                          | 17/45 [05:55<09:35, 20.56s/it]

 40%|█████████████████████████████████████▌                                                        | 18/45 [06:16<09:15, 20.58s/it]

 42%|███████████████████████████████████████▋                                                      | 19/45 [06:36<08:55, 20.60s/it]

 44%|█████████████████████████████████████████▊                                                    | 20/45 [06:57<08:35, 20.60s/it]

 47%|███████████████████████████████████████████▊                                                  | 21/45 [07:17<08:13, 20.58s/it]

 49%|█████████████████████████████████████████████▉                                                | 22/45 [07:38<07:53, 20.59s/it]

 51%|████████████████████████████████████████████████                                              | 23/45 [07:59<07:32, 20.58s/it]

 53%|██████████████████████████████████████████████████▏                                           | 24/45 [08:19<07:12, 20.58s/it]

 56%|████████████████████████████████████████████████████▏                                         | 25/45 [08:40<06:51, 20.57s/it]
{'loss': 0.3939, 'grad_norm': 0.8213444712632458, 'learning_rate': 4.444444444444444e-06, 'epoch': 7.02}


 60%|████████████████████████████████████████████████████████▍                                     | 27/45 [09:21<06:10, 20.61s/it]

 62%|██████████████████████████████████████████████████████████▍                                   | 28/45 [09:42<05:49, 20.58s/it]

 64%|████████████████████████████████████████████████████████████▌                                 | 29/45 [10:02<05:29, 20.60s/it]

 67%|██████████████████████████████████████████████████████████████▋                               | 30/45 [10:23<05:08, 20.55s/it]

 69%|████████████████████████████████████████████████████████████████▊                             | 31/45 [10:43<04:47, 20.57s/it]

 71%|██████████████████████████████████████████████████████████████████▊                           | 32/45 [11:04<04:27, 20.57s/it]

 73%|████████████████████████████████████████████████████████████████████▉                         | 33/45 [11:24<04:06, 20.56s/it]

 76%|███████████████████████████████████████████████████████████████████████                       | 34/45 [11:45<03:46, 20.59s/it]

 78%|█████████████████████████████████████████████████████████████████████████                     | 35/45 [12:06<03:25, 20.57s/it]

 80%|███████████████████████████████████████████████████████████████████████████▏                  | 36/45 [12:26<03:04, 20.54s/it]
{'loss': 0.2202, 'grad_norm': 0.6572116436334914, 'learning_rate': 2.0000000000000003e-06, 'epoch': 10.11}


 84%|███████████████████████████████████████████████████████████████████████████████▍              | 38/45 [13:07<02:23, 20.54s/it]

 87%|█████████████████████████████████████████████████████████████████████████████████▍            | 39/45 [13:28<02:03, 20.55s/it]

 89%|███████████████████████████████████████████████████████████████████████████████████▌          | 40/45 [13:48<01:42, 20.54s/it]
{'loss': 0.1678, 'grad_norm': 0.6362255268973581, 'learning_rate': 1.111111111111111e-06, 'epoch': 11.23}


 93%|███████████████████████████████████████████████████████████████████████████████████████▋      | 42/45 [14:29<01:01, 20.52s/it]

 96%|█████████████████████████████████████████████████████████████████████████████████████████▊    | 43/45 [14:50<00:41, 20.55s/it]

 98%|███████████████████████████████████████████████████████████████████████████████████████████▉  | 44/45 [15:10<00:20, 20.55s/it]
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [15:31<00:00, 20.55s/it][INFO|trainer.py:2383] 2024-08-27 07:23:29,576 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [15:31<00:00, 20.70s/it]
{'loss': 0.1358, 'grad_norm': 0.5361666364718212, 'learning_rate': 0.0, 'epoch': 12.63}
{'train_runtime': 939.1623, 'train_samples_per_second': 14.375, 'train_steps_per_second': 0.048, 'train_loss': 0.4741308682494693, 'epoch': 12.63}
[INFO|trainer.py:3478] 2024-08-27 07:23:36,697 >> Saving model checkpoint to /model/output/llama2-7b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05
[INFO|configuration_utils.py:472] 2024-08-27 07:23:36,701 >> Configuration saved in /model/output/llama2-7b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/config.json
[INFO|configuration_utils.py:769] 2024-08-27 07:23:36,702 >> Configuration saved in /model/output/llama2-7b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-27 07:23:50,927 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/llama2-7b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-27 07:23:50,928 >> tokenizer config file saved in /model/output/llama2-7b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-27 07:23:50,929 >> Special tokens file saved in /model/output/llama2-7b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/special_tokens_map.json
***** train metrics *****
  epoch                    =    12.6316
  total_flos               =    11316GF
  train_loss               =     0.4741
  train_runtime            = 0:15:39.16
  train_samples_per_second =     14.375
  train_steps_per_second   =      0.048
[INFO|trainer.py:3788] 2024-08-27 07:23:51,579 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-27 07:23:51,579 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-27 07:23:51,579 >>   Batch size = 1
 31%|█████████████████████████████▏                                                                 | 4/13 [00:01<00:02,  3.31it/s]
Figure saved at: /model/output/llama2-7b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/training_loss.png

100%|██████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.27it/s]
[INFO|modelcard.py:449] 2024-08-27 07:23:55,919 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =    12.6316
  eval_loss               =      1.576
  eval_runtime            = 0:00:04.33
  eval_samples_per_second =     23.046
  eval_steps_per_second   =      2.996