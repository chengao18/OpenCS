  0%|                                                                                                                                                        | 0/45 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  2%|███▏                                                                                                                                            | 1/45 [00:25<18:46, 25.59s/it]

  4%|██████▍                                                                                                                                         | 2/45 [00:46<16:11, 22.59s/it]

  7%|█████████▌                                                                                                                                      | 3/45 [01:06<15:14, 21.78s/it]
{'loss': 1.1882, 'grad_norm': 5.23739017926797, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.84}


 11%|████████████████                                                                                                                                | 5/45 [01:47<14:00, 21.02s/it]

 13%|███████████████████▏                                                                                                                            | 6/45 [02:08<13:32, 20.83s/it]

 16%|██████████████████████▍                                                                                                                         | 7/45 [02:28<13:07, 20.71s/it]
{'loss': 1.0269, 'grad_norm': 1.6312987689331924, 'learning_rate': 8.444444444444446e-06, 'epoch': 1.96}


 20%|████████████████████████████▊                                                                                                                   | 9/45 [03:09<12:22, 20.62s/it]

 22%|███████████████████████████████▊                                                                                                               | 10/45 [03:30<12:02, 20.65s/it]
{'loss': 0.8479, 'grad_norm': 1.4588181734212966, 'learning_rate': 7.77777777777778e-06, 'epoch': 2.81}

 24%|██████████████████████████████████▉                                                                                                            | 11/45 [03:51<11:41, 20.63s/it]


 29%|█████████████████████████████████████████▎                                                                                                     | 13/45 [04:32<10:58, 20.59s/it]

 31%|████████████████████████████████████████████▍                                                                                                  | 14/45 [04:52<10:37, 20.57s/it]
{'loss': 0.6883, 'grad_norm': 1.092528014794988, 'learning_rate': 6.88888888888889e-06, 'epoch': 3.93}


 36%|██████████████████████████████████████████████████▊                                                                                            | 16/45 [05:34<09:56, 20.57s/it]

 38%|██████████████████████████████████████████████████████                                                                                         | 17/45 [05:54<09:35, 20.57s/it]

 40%|█████████████████████████████████████████████████████████▏                                                                                     | 18/45 [06:15<09:15, 20.56s/it]
{'loss': 0.5142, 'grad_norm': 1.1845833446669514, 'learning_rate': 6e-06, 'epoch': 5.05}


 44%|███████████████████████████████████████████████████████████████▌                                                                               | 20/45 [06:56<08:33, 20.54s/it]

 47%|██████████████████████████████████████████████████████████████████▋                                                                            | 21/45 [07:16<08:12, 20.54s/it]

 49%|█████████████████████████████████████████████████████████████████████▉                                                                         | 22/45 [07:37<07:52, 20.55s/it]
{'loss': 0.4354, 'grad_norm': 1.3465461028458081, 'learning_rate': 5.1111111111111115e-06, 'epoch': 6.18}


 53%|████████████████████████████████████████████████████████████████████████████▎                                                                  | 24/45 [08:18<07:10, 20.49s/it]

 56%|███████████████████████████████████████████████████████████████████████████████▍                                                               | 25/45 [08:38<06:49, 20.50s/it]

 58%|██████████████████████████████████████████████████████████████████████████████████▌                                                            | 26/45 [08:59<06:29, 20.50s/it]
{'loss': 0.3271, 'grad_norm': 1.0058174198622674, 'learning_rate': 4.222222222222223e-06, 'epoch': 7.3}


 62%|████████████████████████████████████████████████████████████████████████████████████████▉                                                      | 28/45 [09:40<05:48, 20.52s/it]

 64%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                  | 29/45 [10:00<05:28, 20.51s/it]

 67%|███████████████████████████████████████████████████████████████████████████████████████████████▎                                               | 30/45 [10:21<05:07, 20.52s/it]
{'loss': 0.2618, 'grad_norm': 0.9539784846968649, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.42}


 71%|█████████████████████████████████████████████████████████████████████████████████████████████████████▋                                         | 32/45 [11:02<04:27, 20.56s/it]

 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                      | 33/45 [11:22<04:06, 20.51s/it]

 76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                   | 34/45 [11:43<03:45, 20.53s/it]

 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                               | 35/45 [12:03<03:24, 20.44s/it]
{'loss': 0.2032, 'grad_norm': 0.8323639039037439, 'learning_rate': 2.222222222222222e-06, 'epoch': 9.82}


 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 37/45 [12:44<02:43, 20.44s/it]

 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                      | 38/45 [13:05<02:23, 20.44s/it]

 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                   | 39/45 [13:25<02:02, 20.46s/it]
{'loss': 0.1679, 'grad_norm': 0.831538954684234, 'learning_rate': 1.3333333333333334e-06, 'epoch': 10.95}


 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎            | 41/45 [14:06<01:21, 20.47s/it]

 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍         | 42/45 [14:26<01:01, 20.47s/it]

 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋      | 43/45 [14:47<00:40, 20.47s/it]
{'loss': 0.1255, 'grad_norm': 0.7769950199483103, 'learning_rate': 4.444444444444445e-07, 'epoch': 12.07}

 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 44/45 [15:08<00:20, 20.50s/it]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [15:28<00:00, 20.52s/it][INFO|trainer.py:2383] 2024-08-28 04:07:25,577 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [15:28<00:00, 20.63s/it]
{'train_runtime': 937.0709, 'train_samples_per_second': 14.407, 'train_steps_per_second': 0.048, 'train_loss': 0.523004048731592, 'epoch': 12.63}
[INFO|trainer.py:3478] 2024-08-28 04:07:32,834 >> Saving model checkpoint to /model/output/llama2-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr1e-05
[INFO|configuration_utils.py:472] 2024-08-28 04:07:32,837 >> Configuration saved in /model/output/llama2-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr1e-05/config.json
[INFO|configuration_utils.py:769] 2024-08-28 04:07:32,837 >> Configuration saved in /model/output/llama2-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr1e-05/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-28 04:07:47,152 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/llama2-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr1e-05/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-28 04:07:47,154 >> tokenizer config file saved in /model/output/llama2-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr1e-05/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-28 04:07:47,154 >> Special tokens file saved in /model/output/llama2-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr1e-05/special_tokens_map.json
[INFO|trainer.py:3788] 2024-08-28 04:07:47,811 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-28 04:07:47,811 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-28 04:07:47,811 >>   Batch size = 1
 15%|██████████████████████▏                                                                                                                         | 2/13 [00:00<00:03,  3.34it/s]
***** train metrics *****
  epoch                    =    12.6316
  total_flos               =     4145GF
  train_loss               =      0.523
  train_runtime            = 0:15:37.07
  train_samples_per_second =     14.407
  train_steps_per_second   =      0.048
Figure saved at: /model/output/llama2-7b-alpaca_tips_WHAT_1k_sharegpt-e15lr1e-05/training_loss.png

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  3.22it/s]
[INFO|modelcard.py:449] 2024-08-28 04:07:52,230 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =    12.6316
  eval_loss               =     1.6022
  eval_runtime            = 0:00:04.41
  eval_samples_per_second =     22.631
  eval_steps_per_second   =      2.942