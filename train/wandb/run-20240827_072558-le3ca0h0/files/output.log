  0%|                                                                                                      | 0/135 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|▋                                                                                           | 1/135 [00:49<1:51:37, 49.98s/it]
{'loss': 1.0085, 'grad_norm': 2.7916047293401456, 'learning_rate': 4.878048780487805e-07, 'epoch': 0.02}


  2%|██                                                                                          | 3/135 [02:24<1:45:30, 47.96s/it]

  3%|██▋                                                                                         | 4/135 [03:12<1:44:16, 47.76s/it]
{'loss': 1.0192, 'grad_norm': 2.6958619373185653, 'learning_rate': 1.951219512195122e-06, 'epoch': 0.09}


  4%|████                                                                                        | 6/135 [04:47<1:42:14, 47.55s/it]

  5%|████▊                                                                                       | 7/135 [05:34<1:41:19, 47.50s/it]
{'loss': 1.0568, 'grad_norm': 2.286605922885692, 'learning_rate': 3.414634146341464e-06, 'epoch': 0.15}

  6%|█████▍                                                                                      | 8/135 [06:21<1:40:32, 47.50s/it]


  7%|██████▋                                                                                    | 10/135 [07:56<1:38:50, 47.44s/it]
{'loss': 0.9424, 'grad_norm': 1.242639807159796, 'learning_rate': 4.8780487804878055e-06, 'epoch': 0.22}

  8%|███████▍                                                                                   | 11/135 [08:44<1:38:01, 47.43s/it]


 10%|████████▊                                                                                  | 13/135 [10:18<1:36:26, 47.43s/it]
{'loss': 0.892, 'grad_norm': 0.8349062182248005, 'learning_rate': 6.341463414634147e-06, 'epoch': 0.28}

 10%|█████████▍                                                                                 | 14/135 [11:06<1:35:35, 47.40s/it]


 12%|██████████▊                                                                                | 16/135 [12:40<1:33:55, 47.35s/it]
{'loss': 0.8747, 'grad_norm': 0.7256506253103315, 'learning_rate': 7.804878048780489e-06, 'epoch': 0.35}

 13%|███████████▍                                                                               | 17/135 [13:28<1:33:11, 47.39s/it]


 14%|████████████▊                                                                              | 19/135 [15:03<1:31:39, 47.41s/it]
{'loss': 0.8686, 'grad_norm': 0.5877689983489045, 'learning_rate': 9.268292682926831e-06, 'epoch': 0.42}

 15%|█████████████▍                                                                             | 20/135 [15:50<1:30:48, 47.38s/it]


 16%|██████████████▊                                                                            | 22/135 [17:25<1:29:16, 47.40s/it]
{'loss': 0.8444, 'grad_norm': 0.502439834399494, 'learning_rate': 1.0731707317073172e-05, 'epoch': 0.48}

 17%|███████████████▌                                                                           | 23/135 [18:12<1:28:31, 47.43s/it]


 19%|████████████████▊                                                                          | 25/135 [19:47<1:26:54, 47.41s/it]
{'loss': 0.8482, 'grad_norm': 0.45873792809134545, 'learning_rate': 1.2195121951219513e-05, 'epoch': 0.55}

 19%|█████████████████▌                                                                         | 26/135 [20:35<1:26:11, 47.44s/it]

 20%|██████████████████▏                                                                        | 27/135 [21:22<1:25:25, 47.46s/it]


 21%|███████████████████▌                                                                       | 29/135 [22:57<1:23:52, 47.48s/it]
{'loss': 0.7744, 'grad_norm': 0.44466061310434135, 'learning_rate': 1.4146341463414635e-05, 'epoch': 0.63}

 22%|████████████████████▏                                                                      | 30/135 [23:45<1:23:06, 47.49s/it]


 24%|█████████████████████▌                                                                     | 32/135 [25:20<1:21:33, 47.51s/it]
{'loss': 0.8502, 'grad_norm': 0.46622232168335415, 'learning_rate': 1.5609756097560978e-05, 'epoch': 0.7}


 25%|██████████████████████▉                                                                    | 34/135 [26:54<1:19:38, 47.31s/it]

 26%|███████████████████████▌                                                                   | 35/135 [27:41<1:18:54, 47.35s/it]
{'loss': 0.806, 'grad_norm': 0.4095781157281885, 'learning_rate': 1.7073170731707317e-05, 'epoch': 0.77}

 27%|████████████████████████▎                                                                  | 36/135 [28:29<1:18:14, 47.42s/it]

 27%|████████████████████████▉                                                                  | 37/135 [29:17<1:17:33, 47.49s/it]


 29%|██████████████████████████▎                                                                | 39/135 [30:52<1:16:00, 47.50s/it]

 30%|██████████████████████████▉                                                                | 40/135 [31:39<1:15:13, 47.51s/it]
{'loss': 0.7766, 'grad_norm': 0.4023843022622491, 'learning_rate': 1.9512195121951222e-05, 'epoch': 0.87}


 31%|████████████████████████████▎                                                              | 42/135 [33:14<1:13:34, 47.47s/it]
{'loss': 0.7758, 'grad_norm': 0.41622597118133553, 'learning_rate': 1.9994415637302545e-05, 'epoch': 0.92}

 32%|████████████████████████████▉                                                              | 43/135 [34:02<1:12:47, 47.47s/it]


 33%|██████████████████████████████▎                                                            | 45/135 [35:36<1:11:08, 47.43s/it]

 34%|███████████████████████████████                                                            | 46/135 [36:24<1:10:19, 47.41s/it]
{'loss': 0.7548, 'grad_norm': 0.40178417440149616, 'learning_rate': 1.9860702539900288e-05, 'epoch': 1.01}


 36%|████████████████████████████████▎                                                          | 48/135 [37:59<1:08:48, 47.46s/it]

 36%|█████████████████████████████████                                                          | 49/135 [38:46<1:07:57, 47.41s/it]
{'loss': 0.7048, 'grad_norm': 0.5372852384319047, 'learning_rate': 1.964469175054377e-05, 'epoch': 1.07}

 37%|█████████████████████████████████▋                                                         | 50/135 [39:33<1:07:12, 47.44s/it]


 39%|███████████████████████████████████                                                        | 52/135 [41:09<1:05:45, 47.54s/it]
{'loss': 0.6934, 'grad_norm': 0.432226672131593, 'learning_rate': 1.9331806110416027e-05, 'epoch': 1.14}

 39%|███████████████████████████████████▋                                                       | 53/135 [41:56<1:04:52, 47.47s/it]


 41%|█████████████████████████████████████                                                      | 55/135 [43:31<1:03:13, 47.42s/it]

 41%|█████████████████████████████████████▋                                                     | 56/135 [44:18<1:02:30, 47.47s/it]
{'loss': 0.7289, 'grad_norm': 0.4680864087484197, 'learning_rate': 1.8769499282066716e-05, 'epoch': 1.22}

 42%|██████████████████████████████████████▍                                                    | 57/135 [45:06<1:01:44, 47.49s/it]


 44%|███████████████████████████████████████▊                                                   | 59/135 [46:41<1:00:02, 47.40s/it]
{'loss': 0.7112, 'grad_norm': 0.4437888329935702, 'learning_rate': 1.8244415603417603e-05, 'epoch': 1.29}

 44%|█████████████████████████████████████████▎                                                   | 60/135 [47:28<59:19, 47.45s/it]


 46%|██████████████████████████████████████████▋                                                  | 62/135 [49:03<57:42, 47.44s/it]
{'loss': 0.6769, 'grad_norm': 0.4062263775339833, 'learning_rate': 1.7636521965473324e-05, 'epoch': 1.36}

 47%|███████████████████████████████████████████▍                                                 | 63/135 [49:50<56:47, 47.33s/it]


 48%|████████████████████████████████████████████▊                                                | 65/135 [51:25<55:20, 47.43s/it]
{'loss': 0.6825, 'grad_norm': 0.4090143019721882, 'learning_rate': 1.6951924276746425e-05, 'epoch': 1.42}


 50%|██████████████████████████████████████████████▏                                              | 67/135 [53:00<53:37, 47.32s/it]

 50%|██████████████████████████████████████████████▊                                              | 68/135 [53:47<52:52, 47.35s/it]
{'loss': 0.6597, 'grad_norm': 0.3914843833434607, 'learning_rate': 1.619749888960245e-05, 'epoch': 1.49}


 52%|████████████████████████████████████████████████▏                                            | 70/135 [55:22<51:18, 47.36s/it]

 53%|████████████████████████████████████████████████▉                                            | 71/135 [56:09<50:33, 47.39s/it]
{'loss': 0.6787, 'grad_norm': 0.4005581017043066, 'learning_rate': 1.5380823531633727e-05, 'epoch': 1.55}


 54%|██████████████████████████████████████████████████▎                                          | 73/135 [57:44<48:56, 47.36s/it]

 55%|██████████████████████████████████████████████████▉                                          | 74/135 [58:31<48:12, 47.41s/it]
{'loss': 0.7087, 'grad_norm': 0.39973748130949793, 'learning_rate': 1.451010119216102e-05, 'epoch': 1.62}


 56%|███████████████████████████████████████████████████▏                                       | 76/135 [1:00:06<46:36, 47.40s/it]

 57%|███████████████████████████████████████████████████▉                                       | 77/135 [1:00:53<45:49, 47.41s/it]
{'loss': 0.636, 'grad_norm': 0.3715837155277613, 'learning_rate': 1.3594077728375129e-05, 'epoch': 1.68}


 59%|█████████████████████████████████████████████████████▎                                     | 79/135 [1:02:28<44:10, 47.33s/it]
{'loss': 0.6968, 'grad_norm': 0.37425406096274005, 'learning_rate': 1.2962755808856341e-05, 'epoch': 1.73}

 59%|█████████████████████████████████████████████████████▉                                     | 80/135 [1:03:15<43:22, 47.32s/it]


 61%|███████████████████████████████████████████████████████▎                                   | 82/135 [1:04:50<41:49, 47.34s/it]
{'loss': 0.6818, 'grad_norm': 0.38699883615949626, 'learning_rate': 1.1991859851038362e-05, 'epoch': 1.79}


 62%|████████████████████████████████████████████████████████▌                                  | 84/135 [1:06:24<40:11, 47.28s/it]

 63%|█████████████████████████████████████████████████████████▎                                 | 85/135 [1:07:12<39:26, 47.33s/it]
{'loss': 0.6986, 'grad_norm': 0.38764126879009614, 'learning_rate': 1.1000956916240985e-05, 'epoch': 1.86}

 64%|█████████████████████████████████████████████████████████▉                                 | 86/135 [1:07:59<38:41, 47.38s/it]


 65%|███████████████████████████████████████████████████████████▎                               | 88/135 [1:09:34<37:06, 47.38s/it]
{'loss': 0.6612, 'grad_norm': 0.40022082566293216, 'learning_rate': 1e-05, 'epoch': 1.92}

 66%|███████████████████████████████████████████████████████████▉                               | 89/135 [1:10:22<36:22, 47.45s/it]


 67%|█████████████████████████████████████████████████████████████▎                             | 91/135 [1:11:57<34:47, 47.44s/it]
{'loss': 0.6861, 'grad_norm': 0.40544128618961417, 'learning_rate': 8.999043083759016e-06, 'epoch': 1.99}

 68%|██████████████████████████████████████████████████████████████                             | 92/135 [1:12:44<33:59, 47.42s/it]


 70%|███████████████████████████████████████████████████████████████▎                           | 94/135 [1:14:19<32:25, 47.45s/it]

 70%|████████████████████████████████████████████████████████████████                           | 95/135 [1:15:06<31:36, 47.42s/it]
{'loss': 0.5551, 'grad_norm': 0.51442411046915, 'learning_rate': 7.681798497324717e-06, 'epoch': 2.08}


 72%|█████████████████████████████████████████████████████████████████▍                         | 97/135 [1:16:41<30:01, 47.40s/it]

 73%|██████████████████████████████████████████████████████████████████                         | 98/135 [1:17:29<29:15, 47.46s/it]
{'loss': 0.5365, 'grad_norm': 0.4602413578050899, 'learning_rate': 6.719751421604309e-06, 'epoch': 2.14}


 74%|██████████████████████████████████████████████████████████████████▋                       | 100/135 [1:19:03<27:38, 47.38s/it]

 75%|███████████████████████████████████████████████████████████████████▎                      | 101/135 [1:19:50<26:50, 47.36s/it]
{'loss': 0.56, 'grad_norm': 0.46475473721733385, 'learning_rate': 5.790652375716653e-06, 'epoch': 2.21}


 76%|████████████████████████████████████████████████████████████████████▋                     | 103/135 [1:21:25<25:16, 47.39s/it]

 77%|█████████████████████████████████████████████████████████████████████▎                    | 104/135 [1:22:13<24:30, 47.43s/it]
{'loss': 0.5608, 'grad_norm': 0.4457766473093296, 'learning_rate': 4.903833574080825e-06, 'epoch': 2.27}


 79%|██████████████████████████████████████████████████████████████████████▋                   | 106/135 [1:23:47<22:52, 47.32s/it]

 79%|███████████████████████████████████████████████████████████████████████▎                  | 107/135 [1:24:35<22:06, 47.38s/it]
{'loss': 0.5707, 'grad_norm': 0.42077337473664855, 'learning_rate': 4.0682025527064486e-06, 'epoch': 2.34}


 81%|████████████████████████████████████████████████████████████████████████▋                 | 109/135 [1:26:10<20:32, 47.42s/it]

 81%|█████████████████████████████████████████████████████████████████████████▎                | 110/135 [1:26:57<19:44, 47.39s/it]
{'loss': 0.5636, 'grad_norm': 0.43019386279482674, 'learning_rate': 3.292152698607768e-06, 'epoch': 2.4}

 82%|██████████████████████████████████████████████████████████████████████████                | 111/135 [1:27:44<18:57, 47.40s/it]

 83%|██████████████████████████████████████████████████████████████████████████▋               | 112/135 [1:28:32<18:11, 47.44s/it]


 84%|████████████████████████████████████████████████████████████████████████████              | 114/135 [1:30:06<16:31, 47.22s/it]

 85%|████████████████████████████████████████████████████████████████████████████▋             | 115/135 [1:30:53<15:46, 47.31s/it]
{'loss': 0.5461, 'grad_norm': 0.378242934570079, 'learning_rate': 2.1520061472133903e-06, 'epoch': 2.51}

 86%|█████████████████████████████████████████████████████████████████████████████▎            | 116/135 [1:31:41<15:00, 47.37s/it]


 87%|██████████████████████████████████████████████████████████████████████████████▋           | 118/135 [1:33:16<13:25, 47.39s/it]

 88%|███████████████████████████████████████████████████████████████████████████████▎          | 119/135 [1:34:03<12:39, 47.46s/it]
{'loss': 0.5874, 'grad_norm': 0.403588951368821, 'learning_rate': 1.3959842073986085e-06, 'epoch': 2.6}


 90%|████████████████████████████████████████████████████████████████████████████████▋         | 121/135 [1:35:38<11:03, 47.41s/it]

 90%|█████████████████████████████████████████████████████████████████████████████████▎        | 122/135 [1:36:26<10:16, 47.44s/it]
{'loss': 0.5416, 'grad_norm': 0.3555351237276894, 'learning_rate': 9.290908626565931e-07, 'epoch': 2.67}

 91%|██████████████████████████████████████████████████████████████████████████████████        | 123/135 [1:37:13<09:29, 47.46s/it]


 93%|███████████████████████████████████████████████████████████████████████████████████▎      | 125/135 [1:38:48<07:54, 47.47s/it]
{'loss': 0.5442, 'grad_norm': 0.3674051167492603, 'learning_rate': 5.533090839208133e-07, 'epoch': 2.73}

 93%|████████████████████████████████████████████████████████████████████████████████████      | 126/135 [1:39:35<07:06, 47.42s/it]


 95%|█████████████████████████████████████████████████████████████████████████████████████▎    | 128/135 [1:41:10<05:31, 47.39s/it]
{'loss': 0.5554, 'grad_norm': 0.36364348615232883, 'learning_rate': 2.7241336234962943e-07, 'epoch': 2.8}


 96%|██████████████████████████████████████████████████████████████████████████████████████▋   | 130/135 [1:42:45<03:56, 47.38s/it]

 97%|███████████████████████████████████████████████████████████████████████████████████████▎  | 131/135 [1:43:32<03:09, 47.37s/it]
{'loss': 0.5483, 'grad_norm': 0.3709493858213542, 'learning_rate': 8.922511845219972e-08, 'epoch': 2.86}


 99%|████████████████████████████████████████████████████████████████████████████████████████▋ | 133/135 [1:45:07<01:34, 47.35s/it]
{'loss': 0.5574, 'grad_norm': 0.37470752516200223, 'learning_rate': 2.2331213768468363e-08, 'epoch': 2.91}

 99%|█████████████████████████████████████████████████████████████████████████████████████████▎| 134/135 [1:45:54<00:47, 47.31s/it]
{'loss': 0.5823, 'grad_norm': 0.3807125101825124, 'learning_rate': 0.0, 'epoch': 2.95}
100%|██████████████████████████████████████████████████████████████████████████████████████████| 135/135 [1:46:42<00:00, 47.38s/it][INFO|trainer.py:2383] 2024-08-27 09:12:47,741 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████| 135/135 [1:46:42<00:00, 47.42s/it]
[INFO|trainer.py:3478] 2024-08-27 09:12:54,858 >> Saving model checkpoint to /model/output/llama2-7b-fastchat_evol_instruct_70k_beautified-e3lr2e-05
[INFO|configuration_utils.py:472] 2024-08-27 09:12:54,861 >> Configuration saved in /model/output/llama2-7b-fastchat_evol_instruct_70k_beautified-e3lr2e-05/config.json
[INFO|configuration_utils.py:769] 2024-08-27 09:12:54,862 >> Configuration saved in /model/output/llama2-7b-fastchat_evol_instruct_70k_beautified-e3lr2e-05/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-27 09:13:09,008 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/llama2-7b-fastchat_evol_instruct_70k_beautified-e3lr2e-05/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-27 09:13:09,009 >> tokenizer config file saved in /model/output/llama2-7b-fastchat_evol_instruct_70k_beautified-e3lr2e-05/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-27 09:13:09,010 >> Special tokens file saved in /model/output/llama2-7b-fastchat_evol_instruct_70k_beautified-e3lr2e-05/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.9508
  total_flos               =   177582GF
  train_loss               =     0.7076
  train_runtime            = 1:46:50.69
  train_samples_per_second =     21.901
  train_steps_per_second   =      0.021
Figure saved at: /model/output/llama2-7b-fastchat_evol_instruct_70k_beautified-e3lr2e-05/training_loss.png
08/27/2024 09:13:09 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
[INFO|trainer.py:3788] 2024-08-27 09:13:09,673 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-27 09:13:09,673 >>   Num examples = 5200
[INFO|trainer.py:3793] 2024-08-27 09:13:09,673 >>   Batch size = 4





























100%|████████████████████████████████████████████████████████████████████████████████████████████| 163/163 [01:00<00:00,  2.70it/s]
[INFO|modelcard.py:449] 2024-08-27 09:14:10,507 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =     2.9508
  eval_loss               =     0.7949
  eval_runtime            = 0:01:00.83
  eval_samples_per_second =     85.484
  eval_steps_per_second   =       2.68