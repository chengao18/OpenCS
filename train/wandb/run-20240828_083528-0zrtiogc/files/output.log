  0%|                                                                                                                                                                    | 0/45 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
{'loss': 6.9051, 'grad_norm': 658.6096150224349, 'learning_rate': 1.9555555555555556e-06, 'epoch': 0.28}
  2%|███▍                                                                                                                                                        | 1/45 [00:28<20:49, 28.40s/it]

  4%|██████▉                                                                                                                                                     | 2/45 [00:51<18:14, 25.44s/it]

  7%|██████████▍                                                                                                                                                 | 3/45 [01:14<17:05, 24.41s/it]

  9%|█████████████▊                                                                                                                                              | 4/45 [01:37<16:16, 23.82s/it]

 11%|█████████████████▎                                                                                                                                          | 5/45 [02:00<15:38, 23.46s/it]

 13%|████████████████████▊                                                                                                                                       | 6/45 [02:23<15:07, 23.28s/it]

 16%|████████████████████████▎                                                                                                                                   | 7/45 [02:46<14:40, 23.17s/it]

 18%|███████████████████████████▋                                                                                                                                | 8/45 [03:09<14:10, 22.99s/it]

 20%|███████████████████████████████▏                                                                                                                            | 9/45 [03:31<13:44, 22.91s/it]

 22%|██████████████████████████████████▍                                                                                                                        | 10/45 [03:54<13:19, 22.83s/it]


 27%|█████████████████████████████████████████▎                                                                                                                 | 12/45 [04:40<12:34, 22.87s/it]
{'loss': 1.8259, 'grad_norm': 43.017983158209546, 'learning_rate': 1.4666666666666665e-06, 'epoch': 3.37}

 29%|████████████████████████████████████████████▊                                                                                                              | 13/45 [05:03<12:10, 22.84s/it]

 31%|████████████████████████████████████████████████▏                                                                                                          | 14/45 [05:25<11:46, 22.80s/it]

 33%|███████████████████████████████████████████████████▋                                                                                                       | 15/45 [05:48<11:25, 22.84s/it]

 36%|███████████████████████████████████████████████████████                                                                                                    | 16/45 [06:11<11:00, 22.78s/it]

 38%|██████████████████████████████████████████████████████████▌                                                                                                | 17/45 [06:34<10:37, 22.75s/it]

 40%|██████████████████████████████████████████████████████████████                                                                                             | 18/45 [06:56<10:14, 22.76s/it]


 44%|████████████████████████████████████████████████████████████████████▉                                                                                      | 20/45 [07:42<09:29, 22.76s/it]
{'loss': 1.4541, 'grad_norm': 31.69689537272588, 'learning_rate': 1.111111111111111e-06, 'epoch': 5.61}

 47%|████████████████████████████████████████████████████████████████████████▎                                                                                  | 21/45 [08:05<09:06, 22.78s/it]

 49%|███████████████████████████████████████████████████████████████████████████▊                                                                               | 22/45 [08:28<08:44, 22.81s/it]

 51%|███████████████████████████████████████████████████████████████████████████████▏                                                                           | 23/45 [08:50<08:21, 22.80s/it]

 53%|██████████████████████████████████████████████████████████████████████████████████▋                                                                        | 24/45 [09:13<07:57, 22.75s/it]

 56%|██████████████████████████████████████████████████████████████████████████████████████                                                                     | 25/45 [09:36<07:35, 22.76s/it]

 58%|█████████████████████████████████████████████████████████████████████████████████████████▌                                                                 | 26/45 [09:59<07:12, 22.76s/it]


 62%|████████████████████████████████████████████████████████████████████████████████████████████████▍                                                          | 28/45 [10:44<06:27, 22.79s/it]
{'loss': 1.1856, 'grad_norm': 23.149120391547143, 'learning_rate': 7.555555555555555e-07, 'epoch': 7.86}

 64%|███████████████████████████████████████████████████████████████████████████████████████████████████▉                                                       | 29/45 [11:07<06:04, 22.79s/it]

 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                   | 30/45 [11:30<05:41, 22.77s/it]

 69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                | 31/45 [11:53<05:19, 22.80s/it]

 71%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                            | 32/45 [12:15<04:56, 22.84s/it]

 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                         | 33/45 [12:38<04:34, 22.85s/it]

 76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                      | 34/45 [13:01<04:11, 22.82s/it]

 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 35/45 [13:24<03:47, 22.80s/it]

 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                               | 36/45 [13:47<03:25, 22.81s/it]


 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                        | 38/45 [14:32<02:39, 22.79s/it]
{'loss': 1.114, 'grad_norm': 24.921390081489236, 'learning_rate': 3.111111111111111e-07, 'epoch': 10.67}

 87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 39/45 [14:55<02:16, 22.75s/it]


 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 41/45 [15:40<01:30, 22.74s/it]
{'loss': 1.0282, 'grad_norm': 26.931423555152655, 'learning_rate': 1.7777777777777776e-07, 'epoch': 11.51}

 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 42/45 [16:03<01:08, 22.78s/it]

 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 43/45 [16:26<00:45, 22.75s/it]

 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 44/45 [16:49<00:22, 22.80s/it]
{'loss': 1.1045, 'grad_norm': 28.28307573526746, 'learning_rate': 0.0, 'epoch': 12.63}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [17:12<00:00, 22.77s/it][INFO|trainer.py:2383] 2024-08-28 08:52:47,008 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [17:12<00:00, 22.93s/it]
[INFO|trainer.py:3478] 2024-08-28 08:52:55,286 >> Saving model checkpoint to /model/output/mistral-7b-alpaca_tips_what_1k_sharegpt_refined-e15lr2e-06
[INFO|configuration_utils.py:472] 2024-08-28 08:52:55,289 >> Configuration saved in /model/output/mistral-7b-alpaca_tips_what_1k_sharegpt_refined-e15lr2e-06/config.json
[INFO|configuration_utils.py:769] 2024-08-28 08:52:55,289 >> Configuration saved in /model/output/mistral-7b-alpaca_tips_what_1k_sharegpt_refined-e15lr2e-06/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-28 08:53:15,954 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/mistral-7b-alpaca_tips_what_1k_sharegpt_refined-e15lr2e-06/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-28 08:53:15,955 >> tokenizer config file saved in /model/output/mistral-7b-alpaca_tips_what_1k_sharegpt_refined-e15lr2e-06/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-28 08:53:15,955 >> Special tokens file saved in /model/output/mistral-7b-alpaca_tips_what_1k_sharegpt_refined-e15lr2e-06/special_tokens_map.json
***** train metrics *****
  epoch                    =    12.6316
  total_flos               =     6203GF
  train_loss               =     1.7576
  train_runtime            = 0:17:20.34
  train_samples_per_second =     12.976
  train_steps_per_second   =      0.043
Figure saved at: /model/output/mistral-7b-alpaca_tips_what_1k_sharegpt_refined-e15lr2e-06/training_loss.png
08/28/2024 08:53:16 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
[INFO|trainer.py:3788] 2024-08-28 08:53:16,660 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-28 08:53:16,660 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-28 08:53:16,660 >>   Batch size = 1

 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                | 9/13 [00:02<00:01,  3.11it/s]
***** eval metrics *****
  epoch                   =    12.6316
  eval_loss               =     1.0282
  eval_runtime            = 0:00:04.62
  eval_samples_per_second =     21.632
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:04<00:00,  3.10it/s]
[INFO|modelcard.py:449] 2024-08-28 08:53:21,283 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}