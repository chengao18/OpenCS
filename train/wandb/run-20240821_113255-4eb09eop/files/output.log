  0%|                                                                                                                                   | 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|█▏                                                                                                                       | 1/105 [00:34<1:00:39, 34.99s/it]

  2%|██▎                                                                                                                        | 2/105 [01:04<54:37, 31.82s/it]

  3%|███▌                                                                                                                       | 3/105 [01:33<52:12, 30.71s/it]

  4%|████                                                                                                       | 4/105 [02:03<50:46, 30.16s/it]

  5%|█████                                                                                                      | 5/105 [02:32<49:55, 29.96s/it]

  6%|██████                                                                                                     | 6/105 [03:02<49:22, 29.93s/it]

  7%|███████▏                                                                                                   | 7/105 [03:31<48:27, 29.67s/it]

  8%|████████▏                                                                                                  | 8/105 [04:01<47:42, 29.51s/it]

  9%|█████████▏                                                                                                 | 9/105 [04:30<47:04, 29.42s/it]

 10%|██████████                                                                                                | 10/105 [04:59<46:37, 29.45s/it]

 10%|███████████                                                                                               | 11/105 [05:29<46:14, 29.52s/it]

 11%|████████████                                                                                              | 12/105 [05:59<45:47, 29.54s/it]

 12%|█████████████                                                                                             | 13/105 [06:28<45:13, 29.49s/it]

 13%|██████████████▏                                                                                           | 14/105 [06:57<44:44, 29.50s/it]

 14%|███████████████▏                                                                                          | 15/105 [07:27<44:17, 29.53s/it]

 15%|████████████████▏                                                                                         | 16/105 [07:57<43:50, 29.56s/it]

 16%|█████████████████▏                                                                                        | 17/105 [08:26<43:16, 29.50s/it]

 17%|██████████████████▏                                                                                       | 18/105 [08:55<42:42, 29.46s/it]

 18%|███████████████████▏                                                                                      | 19/105 [09:25<42:17, 29.50s/it]

 19%|████████████████████▏                                                                                     | 20/105 [09:55<41:55, 29.60s/it]
{'loss': 0.7107, 'grad_norm': 4.238644407684481, 'learning_rate': 8.095238095238097e-06, 'epoch': 2.83}


 21%|██████████████████████▏                                                                                   | 22/105 [10:54<40:57, 29.61s/it]

 22%|███████████████████████▏                                                                                  | 23/105 [11:23<40:22, 29.54s/it]

 23%|████████████████████████▏                                                                                 | 24/105 [11:53<39:53, 29.55s/it]
{'loss': 0.5252, 'grad_norm': 2.2505402446234277, 'learning_rate': 7.714285714285716e-06, 'epoch': 3.4}


 25%|██████████████████████████▏                                                                               | 26/105 [12:52<38:51, 29.51s/it]

 26%|███████████████████████████▎                                                                              | 27/105 [13:21<38:18, 29.46s/it]

 27%|████████████████████████████▎                                                                             | 28/105 [13:51<37:51, 29.49s/it]

 28%|█████████████████████████████▎                                                                            | 29/105 [14:21<37:34, 29.66s/it]

 29%|██████████████████████████████▎                                                                           | 30/105 [14:51<37:02, 29.64s/it]

 30%|███████████████████████████████▎                                                                          | 31/105 [15:20<36:31, 29.61s/it]

 30%|████████████████████████████████▎                                                                         | 32/105 [15:50<35:57, 29.56s/it]

 31%|█████████████████████████████████▎                                                                        | 33/105 [16:19<35:31, 29.61s/it]
{'loss': 0.2825, 'grad_norm': 1.9824345029231425, 'learning_rate': 6.857142857142858e-06, 'epoch': 4.67}


 33%|███████████████████████████████████▎                                                                      | 35/105 [17:18<34:29, 29.56s/it]

 34%|████████████████████████████████████▎                                                                     | 36/105 [17:48<33:56, 29.51s/it]

 35%|█████████████████████████████████████▎                                                                    | 37/105 [18:17<33:25, 29.49s/it]
{'loss': 0.1961, 'grad_norm': 1.719432972548093, 'learning_rate': 6.476190476190477e-06, 'epoch': 5.24}


 37%|███████████████████████████████████████▎                                                                  | 39/105 [19:17<32:32, 29.58s/it]

 38%|████████████████████████████████████████▍                                                                 | 40/105 [19:46<32:02, 29.57s/it]

 39%|█████████████████████████████████████████▍                                                                | 41/105 [20:15<31:27, 29.50s/it]

 40%|██████████████████████████████████████████▍                                                               | 42/105 [20:45<31:04, 29.60s/it]

 41%|███████████████████████████████████████████▍                                                              | 43/105 [21:15<30:35, 29.60s/it]

 42%|████████████████████████████████████████████▍                                                             | 44/105 [21:44<30:01, 29.54s/it]

 43%|█████████████████████████████████████████████▍                                                            | 45/105 [22:13<29:25, 29.42s/it]
{'loss': 0.0945, 'grad_norm': 1.4653346345465879, 'learning_rate': 5.7142857142857145e-06, 'epoch': 6.37}


 45%|███████████████████████████████████████████████▍                                                          | 47/105 [23:13<28:38, 29.62s/it]

 46%|████████████████████████████████████████████████▍                                                         | 48/105 [23:43<28:10, 29.65s/it]

 47%|█████████████████████████████████████████████████▍                                                        | 49/105 [24:12<27:38, 29.61s/it]

 48%|██████████████████████████████████████████████████▍                                                       | 50/105 [24:42<27:04, 29.53s/it]
{'loss': 0.0577, 'grad_norm': 1.4011097528678111, 'learning_rate': 5.2380952380952384e-06, 'epoch': 7.08}

 49%|███████████████████████████████████████████████████▍                                                      | 51/105 [25:11<26:38, 29.61s/it]


 50%|█████████████████████████████████████████████████████▌                                                    | 53/105 [26:11<25:42, 29.66s/it]

 51%|██████████████████████████████████████████████████████▌                                                   | 54/105 [26:41<25:18, 29.77s/it]

 52%|███████████████████████████████████████████████████████▌                                                  | 55/105 [27:10<24:43, 29.66s/it]

 53%|████████████████████████████████████████████████████████▌                                                 | 56/105 [27:40<24:13, 29.67s/it]

 54%|█████████████████████████████████████████████████████████▌                                                | 57/105 [28:09<23:41, 29.61s/it]

 55%|██████████████████████████████████████████████████████████▌                                               | 58/105 [28:39<23:12, 29.63s/it]

 56%|███████████████████████████████████████████████████████████▌                                              | 59/105 [29:08<22:37, 29.51s/it]

 57%|████████████████████████████████████████████████████████████▌                                             | 60/105 [29:38<22:11, 29.58s/it]

 58%|█████████████████████████████████████████████████████████████▌                                            | 61/105 [30:08<21:45, 29.68s/it]
{'loss': 0.0177, 'grad_norm': 1.3472684440461637, 'learning_rate': 4.190476190476191e-06, 'epoch': 8.64}


 60%|███████████████████████████████████████████████████████████████▌                                          | 63/105 [31:07<20:43, 29.60s/it]

 61%|████████████████████████████████████████████████████████████████▌                                         | 64/105 [31:37<20:11, 29.56s/it]

 62%|█████████████████████████████████████████████████████████████████▌                                        | 65/105 [32:07<19:48, 29.71s/it]

 63%|██████████████████████████████████████████████████████████████████▋                                       | 66/105 [32:36<19:18, 29.71s/it]

 64%|███████████████████████████████████████████████████████████████████▋                                      | 67/105 [33:06<18:45, 29.61s/it]

 65%|████████████████████████████████████████████████████████████████████▋                                     | 68/105 [33:35<18:14, 29.58s/it]

 66%|█████████████████████████████████████████████████████████████████████▋                                    | 69/105 [34:05<17:43, 29.56s/it]

 67%|██████████████████████████████████████████████████████████████████████▋                                   | 70/105 [34:34<17:15, 29.59s/it]
{'loss': 0.0211, 'grad_norm': 0.5836429427120321, 'learning_rate': 3.3333333333333333e-06, 'epoch': 9.91}


 69%|████████████████████████████████████████████████████████████████████████▋                                 | 72/105 [35:34<16:17, 29.61s/it]

 70%|█████████████████████████████████████████████████████████████████████████▋                                | 73/105 [36:03<15:45, 29.55s/it]

 70%|██████████████████████████████████████████████████████████████████████████▋                               | 74/105 [36:33<15:17, 29.60s/it]

 71%|███████████████████████████████████████████████████████████████████████████▋                              | 75/105 [37:02<14:48, 29.63s/it]
{'loss': 0.0188, 'grad_norm': 0.8002947189935673, 'learning_rate': 2.8571428571428573e-06, 'epoch': 10.62}


 73%|█████████████████████████████████████████████████████████████████████████████▋                            | 77/105 [38:02<13:49, 29.61s/it]

 74%|██████████████████████████████████████████████████████████████████████████████▋                           | 78/105 [38:31<13:17, 29.56s/it]

 75%|███████████████████████████████████████████████████████████████████████████████▊                          | 79/105 [39:01<12:49, 29.59s/it]
{'loss': 0.0034, 'grad_norm': 0.30578453687910007, 'learning_rate': 2.4761904761904764e-06, 'epoch': 11.19}


 77%|█████████████████████████████████████████████████████████████████████████████████▊                        | 81/105 [40:00<11:50, 29.62s/it]

 78%|██████████████████████████████████████████████████████████████████████████████████▊                       | 82/105 [40:29<11:19, 29.56s/it]

 79%|███████████████████████████████████████████████████████████████████████████████████▊                      | 83/105 [40:59<10:50, 29.56s/it]

 80%|████████████████████████████████████████████████████████████████████████████████████▊                     | 84/105 [41:29<10:21, 29.62s/it]
{'loss': 0.0038, 'grad_norm': 0.4193156990387525, 'learning_rate': 2.0000000000000003e-06, 'epoch': 11.89}


 82%|██████████████████████████████████████████████████████████████████████████████████████▊                   | 86/105 [42:28<09:21, 29.53s/it]

 83%|███████████████████████████████████████████████████████████████████████████████████████▊                  | 87/105 [42:57<08:50, 29.49s/it]

 84%|████████████████████████████████████████████████████████████████████████████████████████▊                 | 88/105 [43:27<08:22, 29.58s/it]
{'loss': 0.0032, 'grad_norm': 0.6801540483561347, 'learning_rate': 1.6190476190476193e-06, 'epoch': 12.46}


 86%|██████████████████████████████████████████████████████████████████████████████████████████▊               | 90/105 [44:26<07:23, 29.55s/it]

 87%|███████████████████████████████████████████████████████████████████████████████████████████▊              | 91/105 [44:56<06:53, 29.54s/it]

 88%|████████████████████████████████████████████████████████████████████████████████████████████▉             | 92/105 [45:25<06:24, 29.56s/it]

 89%|█████████████████████████████████████████████████████████████████████████████████████████████▉            | 93/105 [45:55<05:55, 29.62s/it]
{'loss': 0.0033, 'grad_norm': 2.3679689433149047, 'learning_rate': 1.142857142857143e-06, 'epoch': 13.17}


 90%|███████████████████████████████████████████████████████████████████████████████████████████████▉          | 95/105 [46:54<04:56, 29.67s/it]

 91%|████████████████████████████████████████████████████████████████████████████████████████████████▉         | 96/105 [47:24<04:25, 29.54s/it]

 92%|█████████████████████████████████████████████████████████████████████████████████████████████████▉        | 97/105 [47:53<03:57, 29.63s/it]

 93%|██████████████████████████████████████████████████████████████████████████████████████████████████▉       | 98/105 [48:23<03:27, 29.68s/it]
{'loss': 0.002, 'grad_norm': 0.3522775689369858, 'learning_rate': 6.666666666666667e-07, 'epoch': 13.88}


 95%|████████████████████████████████████████████████████████████████████████████████████████████████████     | 100/105 [49:22<02:28, 29.61s/it]

 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████    | 101/105 [49:52<01:58, 29.59s/it]

 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████   | 102/105 [50:22<01:28, 29.66s/it]

 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████  | 103/105 [50:51<00:59, 29.62s/it]

 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████ | 104/105 [51:21<00:29, 29.62s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [51:50<00:00, 29.51s/it][INFO|trainer.py:2383] 2024-08-21 12:24:52,920 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [51:50<00:00, 29.62s/it]
{'loss': 0.0016, 'grad_norm': 0.07849644633107423, 'learning_rate': 0.0, 'epoch': 14.87}
{'train_runtime': 3118.3345, 'train_samples_per_second': 4.329, 'train_steps_per_second': 0.034, 'train_loss': 0.31556581672879735, 'epoch': 14.87}
[INFO|trainer.py:3478] 2024-08-21 12:25:00,586 >> Saving model checkpoint to /model/output/llama2-7b-filtered_alpaca_1k_score_beautified-e15lr1e-5
[INFO|configuration_utils.py:472] 2024-08-21 12:25:00,589 >> Configuration saved in /model/output/llama2-7b-filtered_alpaca_1k_score_beautified-e15lr1e-5/config.json
[INFO|configuration_utils.py:769] 2024-08-21 12:25:00,590 >> Configuration saved in /model/output/llama2-7b-filtered_alpaca_1k_score_beautified-e15lr1e-5/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-21 12:25:14,347 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/llama2-7b-filtered_alpaca_1k_score_beautified-e15lr1e-5/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-21 12:25:14,348 >> tokenizer config file saved in /model/output/llama2-7b-filtered_alpaca_1k_score_beautified-e15lr1e-5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-21 12:25:14,349 >> Special tokens file saved in /model/output/llama2-7b-filtered_alpaca_1k_score_beautified-e15lr1e-5/special_tokens_map.json
[INFO|trainer.py:3788] 2024-08-21 12:25:15,143 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-21 12:25:15,143 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-21 12:25:15,144 >>   Batch size = 1
  0%|                                                                                                                    | 0/25 [00:00<?, ?it/s]
***** train metrics *****
  epoch                    =    14.8673
  total_flos               =     3236GF
  train_loss               =     0.3156
  train_runtime            = 0:51:58.33
  train_samples_per_second =      4.329
  train_steps_per_second   =      0.034
Figure saved at: /model/output/llama2-7b-filtered_alpaca_1k_score_beautified-e15lr1e-5/training_loss.png




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:10<00:00,  2.46it/s]
[INFO|modelcard.py:449] 2024-08-21 12:25:25,801 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =    14.8673
  eval_loss               =     2.8811
  eval_runtime            = 0:00:10.65
  eval_samples_per_second =      9.384
  eval_steps_per_second   =      2.346