  0%|                                                                                                       | 0/45 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  2%|██                                                                                             | 1/45 [00:43<31:39, 43.17s/it]
{'loss': 1.1, 'grad_norm': 3.497773552231887, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.28}

  4%|████▏                                                                                          | 2/45 [01:20<28:16, 39.45s/it]

  7%|██████▎                                                                                        | 3/45 [01:56<26:48, 38.30s/it]

  9%|████████▍                                                                                      | 4/45 [02:34<26:02, 38.11s/it]

 11%|██████████▌                                                                                    | 5/45 [03:20<27:08, 40.72s/it]


 16%|██████████████▊                                                                                | 7/45 [04:39<25:17, 39.92s/it]
{'loss': 0.8368, 'grad_norm': 0.987438611645474, 'learning_rate': 8.444444444444446e-06, 'epoch': 1.96}

 18%|████████████████▉                                                                              | 8/45 [05:16<23:57, 38.85s/it]

 20%|███████████████████                                                                            | 9/45 [05:52<22:50, 38.08s/it]


 24%|██████████████████████▉                                                                       | 11/45 [07:05<21:08, 37.32s/it]
{'loss': 0.583, 'grad_norm': 0.8394068801517932, 'learning_rate': 7.555555555555556e-06, 'epoch': 3.09}

 27%|█████████████████████████                                                                     | 12/45 [07:42<20:23, 37.09s/it]

 29%|███████████████████████████▏                                                                  | 13/45 [08:18<19:40, 36.90s/it]


 33%|███████████████████████████████▎                                                              | 15/45 [09:31<18:20, 36.69s/it]
{'loss': 0.4622, 'grad_norm': 0.7166856892596896, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.21}

 36%|█████████████████████████████████▍                                                            | 16/45 [10:08<17:44, 36.70s/it]


 40%|█████████████████████████████████████▌                                                        | 18/45 [11:22<16:31, 36.72s/it]
{'loss': 0.3971, 'grad_norm': 0.897693989809477, 'learning_rate': 6e-06, 'epoch': 5.05}

 42%|███████████████████████████████████████▋                                                      | 19/45 [11:58<15:53, 36.66s/it]

 44%|█████████████████████████████████████████▊                                                    | 20/45 [12:35<15:16, 36.66s/it]


 49%|█████████████████████████████████████████████▉                                                | 22/45 [13:48<14:01, 36.58s/it]
{'loss': 0.2419, 'grad_norm': 0.6841072333824052, 'learning_rate': 5.1111111111111115e-06, 'epoch': 6.18}

 51%|████████████████████████████████████████████████                                              | 23/45 [14:24<13:24, 36.59s/it]

 53%|██████████████████████████████████████████████████▏                                           | 24/45 [15:01<12:47, 36.55s/it]

 56%|████████████████████████████████████████████████████▏                                         | 25/45 [15:37<12:08, 36.43s/it]

 58%|██████████████████████████████████████████████████████▎                                       | 26/45 [16:14<11:33, 36.49s/it]

 60%|████████████████████████████████████████████████████████▍                                     | 27/45 [16:50<10:57, 36.51s/it]

 62%|██████████████████████████████████████████████████████████▍                                   | 28/45 [17:26<10:19, 36.45s/it]

 64%|████████████████████████████████████████████████████████████▌                                 | 29/45 [18:03<09:43, 36.45s/it]

 67%|██████████████████████████████████████████████████████████████▋                               | 30/45 [18:39<09:06, 36.41s/it]

 69%|████████████████████████████████████████████████████████████████▊                             | 31/45 [19:16<08:30, 36.46s/it]

 71%|██████████████████████████████████████████████████████████████████▊                           | 32/45 [19:52<07:54, 36.51s/it]

 73%|████████████████████████████████████████████████████████████████████▉                         | 33/45 [20:29<07:18, 36.55s/it]

 76%|███████████████████████████████████████████████████████████████████████                       | 34/45 [21:05<06:41, 36.51s/it]


 80%|███████████████████████████████████████████████████████████████████████████▏                  | 36/45 [22:18<05:28, 36.46s/it]
{'loss': 0.1084, 'grad_norm': 0.46493433222029334, 'learning_rate': 2.0000000000000003e-06, 'epoch': 10.11}

 82%|█████████████████████████████████████████████████████████████████████████████▎                | 37/45 [22:55<04:51, 36.42s/it]

 84%|███████████████████████████████████████████████████████████████████████████████▍              | 38/45 [23:31<04:15, 36.49s/it]

 87%|█████████████████████████████████████████████████████████████████████████████████▍            | 39/45 [24:08<03:38, 36.46s/it]

 89%|███████████████████████████████████████████████████████████████████████████████████▌          | 40/45 [24:44<03:02, 36.49s/it]

 91%|█████████████████████████████████████████████████████████████████████████████████████▋        | 41/45 [25:21<02:26, 36.52s/it]

 93%|███████████████████████████████████████████████████████████████████████████████████████▋      | 42/45 [25:57<01:49, 36.42s/it]

 96%|█████████████████████████████████████████████████████████████████████████████████████████▊    | 43/45 [26:33<01:12, 36.39s/it]

 98%|███████████████████████████████████████████████████████████████████████████████████████████▉  | 44/45 [27:09<00:36, 36.34s/it]
{'loss': 0.0547, 'grad_norm': 0.31903297448362444, 'learning_rate': 0.0, 'epoch': 12.63}
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [27:46<00:00, 36.34s/it][INFO|trainer.py:2383] 2024-08-27 10:28:12,079 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [27:46<00:00, 37.03s/it]
[INFO|trainer.py:3478] 2024-08-27 10:28:25,959 >> Saving model checkpoint to /model/output/llama2-13b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05
[INFO|configuration_utils.py:472] 2024-08-27 10:28:25,962 >> Configuration saved in /model/output/llama2-13b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/config.json
[INFO|configuration_utils.py:769] 2024-08-27 10:28:25,963 >> Configuration saved in /model/output/llama2-13b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-27 10:28:54,777 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 6 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/llama2-13b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-27 10:28:54,778 >> tokenizer config file saved in /model/output/llama2-13b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-27 10:28:54,779 >> Special tokens file saved in /model/output/llama2-13b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/special_tokens_map.json
***** train metrics *****
  epoch                    =    12.6316
  total_flos               =    17627GF
  train_loss               =     0.3649
  train_runtime            = 0:27:55.24
  train_samples_per_second =      8.059
  train_steps_per_second   =      0.027
Figure saved at: /model/output/llama2-13b-filtered_evol_instruct_1k_score_beautified-e15lr1e-05/training_loss.png
08/27/2024 10:28:55 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
[INFO|trainer.py:3788] 2024-08-27 10:28:55,940 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-27 10:28:55,940 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-27 10:28:55,940 >>   Batch size = 1


 77%|████████████████████████████████████████████████████████████████████████▎                     | 10/13 [00:04<00:01,  2.22it/s]
***** eval metrics *****
  epoch                   =    12.6316
  eval_loss               =     1.5858
  eval_runtime            = 0:00:06.30
  eval_samples_per_second =     15.868
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:05<00:00,  2.27it/s]
[INFO|modelcard.py:449] 2024-08-27 10:29:02,242 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}