  0%|                                                                                                              | 0/45 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  2%|██▎                                                                                                   | 1/45 [00:25<18:38, 25.41s/it]

  4%|████▌                                                                                                 | 2/45 [00:46<16:16, 22.72s/it]
{'loss': 1.4238, 'grad_norm': 11.354604692788392, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.56}

  7%|██████▊                                                                                               | 3/45 [01:06<15:13, 21.75s/it]

  9%|█████████                                                                                             | 4/45 [01:27<14:26, 21.14s/it]

 11%|███████████▎                                                                                          | 5/45 [01:47<13:57, 20.93s/it]


 16%|███████████████▊                                                                                      | 7/45 [02:28<13:05, 20.67s/it]
{'loss': 0.9112, 'grad_norm': 3.53233307289477, 'learning_rate': 8.444444444444446e-06, 'epoch': 1.96}

 18%|██████████████████▏                                                                                   | 8/45 [02:48<12:42, 20.60s/it]

 20%|████████████████████▍                                                                                 | 9/45 [03:09<12:19, 20.54s/it]

 22%|██████████████████████▍                                                                              | 10/45 [03:29<11:58, 20.53s/it]

 24%|████████████████████████▋                                                                            | 11/45 [03:50<11:36, 20.49s/it]

 27%|██████████████████████████▉                                                                          | 12/45 [04:10<11:16, 20.49s/it]

 29%|█████████████████████████████▏                                                                       | 13/45 [04:31<10:54, 20.47s/it]

 31%|███████████████████████████████▍                                                                     | 14/45 [04:51<10:34, 20.47s/it]


 36%|███████████████████████████████████▉                                                                 | 16/45 [05:32<09:53, 20.46s/it]

 38%|██████████████████████████████████████▏                                                              | 17/45 [05:52<09:32, 20.43s/it]
{'loss': 0.4287, 'grad_norm': 2.0298004847230633, 'learning_rate': 6.222222222222223e-06, 'epoch': 4.77}

 40%|████████████████████████████████████████▍                                                            | 18/45 [06:13<09:11, 20.44s/it]

 42%|██████████████████████████████████████████▋                                                          | 19/45 [06:33<08:50, 20.42s/it]


 47%|███████████████████████████████████████████████▏                                                     | 21/45 [07:14<08:10, 20.45s/it]

 49%|█████████████████████████████████████████████████▍                                                   | 22/45 [07:35<07:50, 20.47s/it]
{'loss': 0.3285, 'grad_norm': 1.300398074237896, 'learning_rate': 5.1111111111111115e-06, 'epoch': 6.18}

 51%|███████████████████████████████████████████████████▌                                                 | 23/45 [07:55<07:30, 20.49s/it]


 56%|████████████████████████████████████████████████████████                                             | 25/45 [08:36<06:49, 20.48s/it]

 58%|██████████████████████████████████████████████████████████▎                                          | 26/45 [08:57<06:29, 20.50s/it]
{'loss': 0.2056, 'grad_norm': 1.3970196745025696, 'learning_rate': 4.222222222222223e-06, 'epoch': 7.3}

 60%|████████████████████████████████████████████████████████████▌                                        | 27/45 [09:17<06:08, 20.46s/it]

 62%|██████████████████████████████████████████████████████████████▊                                      | 28/45 [09:38<05:48, 20.48s/it]


 67%|███████████████████████████████████████████████████████████████████▎                                 | 30/45 [10:19<05:06, 20.46s/it]
{'loss': 0.138, 'grad_norm': 1.1688198191141945, 'learning_rate': 3.3333333333333333e-06, 'epoch': 8.42}

 69%|█████████████████████████████████████████████████████████████████████▌                               | 31/45 [10:39<04:46, 20.46s/it]

 71%|███████████████████████████████████████████████████████████████████████▊                             | 32/45 [10:59<04:25, 20.45s/it]


 76%|████████████████████████████████████████████████████████████████████████████▎                        | 34/45 [11:40<03:45, 20.47s/it]

 78%|██████████████████████████████████████████████████████████████████████████████▌                      | 35/45 [12:01<03:24, 20.47s/it]
{'loss': 0.1047, 'grad_norm': 1.120458921602972, 'learning_rate': 2.222222222222222e-06, 'epoch': 9.82}

 80%|████████████████████████████████████████████████████████████████████████████████▊                    | 36/45 [12:21<03:04, 20.47s/it]

 82%|███████████████████████████████████████████████████████████████████████████████████                  | 37/45 [12:42<02:43, 20.44s/it]


 87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 39/45 [13:23<02:02, 20.43s/it]

 89%|█████████████████████████████████████████████████████████████████████████████████████████▊           | 40/45 [13:43<01:41, 20.40s/it]
{'loss': 0.0929, 'grad_norm': 1.471737764930055, 'learning_rate': 1.111111111111111e-06, 'epoch': 11.23}

 91%|████████████████████████████████████████████████████████████████████████████████████████████         | 41/45 [14:03<01:21, 20.45s/it]

 93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 42/45 [14:24<01:01, 20.43s/it]


 98%|██████████████████████████████████████████████████████████████████████████████████████████████████▊  | 44/45 [15:05<00:20, 20.44s/it]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [15:25<00:00, 20.42s/it][INFO|trainer.py:2383] 2024-09-03 09:56:48,975 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [15:25<00:00, 20.57s/it]
{'loss': 0.063, 'grad_norm': 1.4564824029374774, 'learning_rate': 0.0, 'epoch': 12.63}
{'train_runtime': 933.7923, 'train_samples_per_second': 14.457, 'train_steps_per_second': 0.048, 'train_loss': 0.4271663951377074, 'epoch': 12.63}
[INFO|trainer.py:3478] 2024-09-03 09:56:56,101 >> Saving model checkpoint to /model/output/llama2-7b-alpaca_tips_wo_tag_cluster_1k_sharegpt-e15lr1e-05
[INFO|configuration_utils.py:472] 2024-09-03 09:56:56,104 >> Configuration saved in /model/output/llama2-7b-alpaca_tips_wo_tag_cluster_1k_sharegpt-e15lr1e-05/config.json
[INFO|configuration_utils.py:769] 2024-09-03 09:56:56,104 >> Configuration saved in /model/output/llama2-7b-alpaca_tips_wo_tag_cluster_1k_sharegpt-e15lr1e-05/generation_config.json
[INFO|modeling_utils.py:2698] 2024-09-03 09:57:21,993 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/llama2-7b-alpaca_tips_wo_tag_cluster_1k_sharegpt-e15lr1e-05/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-09-03 09:57:21,996 >> tokenizer config file saved in /model/output/llama2-7b-alpaca_tips_wo_tag_cluster_1k_sharegpt-e15lr1e-05/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-09-03 09:57:21,996 >> Special tokens file saved in /model/output/llama2-7b-alpaca_tips_wo_tag_cluster_1k_sharegpt-e15lr1e-05/special_tokens_map.json
[INFO|trainer.py:3788] 2024-09-03 09:57:22,773 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-09-03 09:57:22,773 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-09-03 09:57:22,774 >>   Batch size = 1
  0%|                                                                                                              | 0/13 [00:00<?, ?it/s]
***** train metrics *****
  epoch                    =    12.6316
  total_flos               =     2235GF
  train_loss               =     0.4272
  train_runtime            = 0:15:33.79
  train_samples_per_second =     14.457
  train_steps_per_second   =      0.048
Figure saved at: /model/output/llama2-7b-alpaca_tips_wo_tag_cluster_1k_sharegpt-e15lr1e-05/training_loss.png

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.25it/s]
[INFO|modelcard.py:449] 2024-09-03 09:57:27,154 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =    12.6316
  eval_loss               =     1.2402
  eval_runtime            = 0:00:04.37
  eval_samples_per_second =     22.842
  eval_steps_per_second   =      2.969