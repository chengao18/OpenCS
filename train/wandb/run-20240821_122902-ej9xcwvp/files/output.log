  0%|                                                                            | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  2%|█▏                                                                  | 1/60 [00:28<28:24, 28.89s/it]

  3%|██▎                                                                 | 2/60 [00:53<25:21, 26.24s/it]

  5%|███▍                                                                | 3/60 [01:17<24:03, 25.33s/it]

  7%|████▌                                                               | 4/60 [01:41<23:16, 24.94s/it]

  8%|█████▋                                                              | 5/60 [02:05<22:33, 24.61s/it]

 10%|██████▊                                                             | 6/60 [02:29<21:57, 24.40s/it]

 12%|███████▉                                                            | 7/60 [02:53<21:26, 24.26s/it]

 13%|█████████                                                           | 8/60 [03:17<20:56, 24.17s/it]

 15%|██████████▏                                                         | 9/60 [03:41<20:26, 24.06s/it]

 17%|███████████▏                                                       | 10/60 [04:05<19:59, 23.99s/it]

 18%|████████████▎                                                      | 11/60 [04:29<19:30, 23.90s/it]

 20%|█████████████▍                                                     | 12/60 [04:53<19:06, 23.88s/it]

 22%|██████████████▌                                                    | 13/60 [05:16<18:41, 23.87s/it]
{'loss': 0.7807, 'grad_norm': 7.698352379828665, 'learning_rate': 7.833333333333333e-06, 'epoch': 3.2}

 23%|███████████████▋                                                   | 14/60 [05:40<18:17, 23.86s/it]


 27%|█████████████████▊                                                 | 16/60 [06:28<17:30, 23.87s/it]

 28%|██████████████████▉                                                | 17/60 [06:52<17:06, 23.87s/it]

 30%|████████████████████                                               | 18/60 [07:16<16:40, 23.83s/it]

 32%|█████████████████████▏                                             | 19/60 [07:39<16:18, 23.86s/it]

 33%|██████████████████████▎                                            | 20/60 [08:03<15:54, 23.87s/it]

 35%|███████████████████████▍                                           | 21/60 [08:27<15:31, 23.88s/it]

 37%|████████████████████████▌                                          | 22/60 [08:51<15:07, 23.89s/it]

 38%|█████████████████████████▋                                         | 23/60 [09:15<14:42, 23.85s/it]

 40%|██████████████████████████▊                                        | 24/60 [09:39<14:16, 23.79s/it]
{'loss': 0.3679, 'grad_norm': 2.479594880208962, 'learning_rate': 6e-06, 'epoch': 5.91}


 43%|█████████████████████████████                                      | 26/60 [10:26<13:29, 23.80s/it]

 45%|██████████████████████████████▏                                    | 27/60 [10:50<13:04, 23.76s/it]

 47%|███████████████████████████████▎                                   | 28/60 [11:14<12:40, 23.77s/it]

 48%|████████████████████████████████▍                                  | 29/60 [11:38<12:17, 23.79s/it]

 50%|█████████████████████████████████▌                                 | 30/60 [12:01<11:54, 23.82s/it]

 52%|██████████████████████████████████▌                                | 31/60 [12:25<11:30, 23.81s/it]

 53%|███████████████████████████████████▋                               | 32/60 [12:49<11:06, 23.81s/it]

 55%|████████████████████████████████████▊                              | 33/60 [13:13<10:42, 23.78s/it]

 57%|█████████████████████████████████████▉                             | 34/60 [13:36<10:17, 23.75s/it]

 58%|███████████████████████████████████████                            | 35/60 [14:00<09:53, 23.76s/it]

 60%|████████████████████████████████████████▏                          | 36/60 [14:24<09:29, 23.75s/it]

 62%|█████████████████████████████████████████▎                         | 37/60 [14:48<09:07, 23.79s/it]

 63%|██████████████████████████████████████████▍                        | 38/60 [15:12<08:44, 23.83s/it]

 65%|███████████████████████████████████████████▌                       | 39/60 [15:36<08:21, 23.87s/it]

 67%|████████████████████████████████████████████▋                      | 40/60 [15:59<07:57, 23.86s/it]

 68%|█████████████████████████████████████████████▊                     | 41/60 [16:23<07:32, 23.84s/it]

 70%|██████████████████████████████████████████████▉                    | 42/60 [16:47<07:09, 23.84s/it]
{'loss': 0.0637, 'grad_norm': 1.3321108976027345, 'learning_rate': 3e-06, 'epoch': 10.34}


 73%|█████████████████████████████████████████████████▏                 | 44/60 [17:35<06:21, 23.83s/it]

 75%|██████████████████████████████████████████████████▎                | 45/60 [17:59<05:57, 23.82s/it]

 77%|███████████████████████████████████████████████████▎               | 46/60 [18:22<05:33, 23.82s/it]

 78%|████████████████████████████████████████████████████▍              | 47/60 [18:46<05:09, 23.81s/it]

 80%|█████████████████████████████████████████████████████▌             | 48/60 [19:10<04:45, 23.83s/it]

 82%|██████████████████████████████████████████████████████▋            | 49/60 [19:34<04:22, 23.82s/it]

 83%|███████████████████████████████████████████████████████▊           | 50/60 [19:58<03:58, 23.83s/it]

 85%|████████████████████████████████████████████████████████▉          | 51/60 [20:21<03:34, 23.82s/it]

 87%|██████████████████████████████████████████████████████████         | 52/60 [20:45<03:10, 23.79s/it]

 88%|███████████████████████████████████████████████████████████▏       | 53/60 [21:09<02:46, 23.79s/it]

 90%|████████████████████████████████████████████████████████████▎      | 54/60 [21:33<02:22, 23.80s/it]

 92%|█████████████████████████████████████████████████████████████▍     | 55/60 [21:57<01:59, 23.83s/it]

 93%|██████████████████████████████████████████████████████████████▌    | 56/60 [22:20<01:35, 23.82s/it]

 95%|███████████████████████████████████████████████████████████████▋   | 57/60 [22:44<01:11, 23.82s/it]

 97%|████████████████████████████████████████████████████████████████▊  | 58/60 [23:08<00:47, 23.79s/it]

 98%|█████████████████████████████████████████████████████████████████▉ | 59/60 [23:32<00:23, 23.79s/it]
100%|███████████████████████████████████████████████████████████████████| 60/60 [23:56<00:00, 23.77s/it][INFO|trainer.py:2383] 2024-08-21 12:53:05,728 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████| 60/60 [23:56<00:00, 23.93s/it]
{'loss': 0.0168, 'grad_norm': 0.9672134888936564, 'learning_rate': 0.0, 'epoch': 14.77}
{'train_runtime': 1444.2464, 'train_samples_per_second': 9.347, 'train_steps_per_second': 0.042, 'train_loss': 0.4319103027228266, 'epoch': 14.77}
[INFO|trainer.py:3478] 2024-08-21 12:53:12,988 >> Saving model checkpoint to /model/output/llama2-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-5
[INFO|configuration_utils.py:472] 2024-08-21 12:53:12,990 >> Configuration saved in /model/output/llama2-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-5/config.json
[INFO|configuration_utils.py:769] 2024-08-21 12:53:12,991 >> Configuration saved in /model/output/llama2-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-5/generation_config.json
[INFO|modeling_utils.py:2698] 2024-08-21 12:53:27,047 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /model/output/llama2-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-5/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2574] 2024-08-21 12:53:27,048 >> tokenizer config file saved in /model/output/llama2-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2583] 2024-08-21 12:53:27,049 >> Special tokens file saved in /model/output/llama2-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-5/special_tokens_map.json
[INFO|trainer.py:3788] 2024-08-21 12:53:27,730 >>
***** Running Evaluation *****
[INFO|trainer.py:3790] 2024-08-21 12:53:27,730 >>   Num examples = 100
[INFO|trainer.py:3793] 2024-08-21 12:53:27,730 >>   Batch size = 1
 20%|█████████████▌                                                      | 3/15 [00:00<00:03,  3.13it/s]
***** train metrics *****
  epoch                    =    14.7692
  total_flos               =     2297GF
  train_loss               =     0.4319
  train_runtime            = 0:24:04.24
  train_samples_per_second =      9.347
  train_steps_per_second   =      0.042
Figure saved at: /model/output/llama2-7b-fastchat_alpaca_1k_cluster_beautified-e15lr1e-5/training_loss.png

100%|███████████████████████████████████████████████████████████████████| 15/15 [00:04<00:00,  3.06it/s]
[INFO|modelcard.py:449] 2024-08-21 12:53:33,071 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
***** eval metrics *****
  epoch                   =    14.7692
  eval_loss               =      2.214
  eval_runtime            = 0:00:05.34
  eval_samples_per_second =     18.726
  eval_steps_per_second   =      2.809